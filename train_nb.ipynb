{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madrian-dendorfer\u001b[0m (\u001b[33madrian_s_playground\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import argparse \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import json as json\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "import models as models\n",
    "\n",
    "import wandb\n",
    "# from os import Path\n",
    "\n",
    "import models \n",
    "import datasets\n",
    "import dataset\n",
    "\n",
    "import numpy as np\n",
    "import time as time \n",
    "import util.misc as misc\n",
    "# from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "from util.callbacks import EarlyStop\n",
    "\n",
    "from util.engine_train import train_one_epoch, evaluate # evaluate_online\n",
    "\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser(\"NN training\")\n",
    "\n",
    "    parser.add_argument('--batch_size', default=64, type=int)\n",
    "    parser.add_argument('--epochs', default=400, type=int)\n",
    "    parser.add_argument('--acum_iter', default=1, type=int) \n",
    "\n",
    "    parser.add_argument('--model', default='shallow_conv_net', type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "    \n",
    "    # Model parameters\n",
    "    parser.add_argument('--input_channels', type=int, default=1, metavar='N',\n",
    "                        help='input channels')\n",
    "    parser.add_argument('--input_electrodes', type=int, default=61, metavar='N',\n",
    "                        help='input electrodes')\n",
    "    parser.add_argument('--time_steps', type=int, default=100, metavar='N',\n",
    "                        help='input length')\n",
    "    # parser.add_argument('--length_samples', default=200, \n",
    "    #                     help='length of samples') \n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument('--optimizer', type=str, default=\"adam_w\", \n",
    "                        help='optimizer type') \n",
    "    parser.add_argument('--lr', type=float, default=1e-3, metavar='LR',\n",
    "                        help='learning rate') \n",
    "\n",
    "    # Callback parameters\n",
    "    parser.add_argument('--patience', default=-1, type=float,\n",
    "                        help='Early stopping whether val is worse than train for specified nb of epochs (default: -1, i.e. no early stopping)')\n",
    "    parser.add_argument('--max_delta', default=0, type=float,\n",
    "                        help='Early stopping threshold (val has to be worse than (train+delta)) (default: 0)')\n",
    "\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument('--data_path', \n",
    "                        # default='_.pt',\n",
    "                        default=\"/vol/aimspace/users/dena/Documents/mae/data/lemon/data_raw_train.pt\",\n",
    "                        type=str,\n",
    "                        help='train dataset path')\n",
    "\n",
    "    parser.add_argument('--labels_path', \n",
    "                        # default='_.pt', \n",
    "                        default=\"/vol/aimspace/users/dena/Documents/ad_benchmarking/ad_benchmarking/data/labels_bin_train.pt\", #labels_raw_train.pt\",\n",
    "                        type=str,\n",
    "                        help='train labels path')\n",
    "    parser.add_argument('--val_data_path', \n",
    "                        # default='', \n",
    "                        default=\"/vol/aimspace/users/dena/Documents/mae/data/lemon/data_raw_val.pt\",\n",
    "                        type=str,\n",
    "                        help='validation dataset path')\n",
    "    parser.add_argument('--val_labels_path', \n",
    "                        # default='_.pt', \n",
    "                        default=\"/vol/aimspace/users/dena/Documents/ad_benchmarking/ad_benchmarking/data/labels_bin_val.pt\", # \"labels_raw_val.pt\"\n",
    "                        type=str,\n",
    "                        help='validation labels path')\n",
    "    parser.add_argument('--number_samples', default=1, type=int, # | str, \n",
    "                        help='number of samples on which network should train on. \"None\" means all samples.')\n",
    "    \n",
    "    \n",
    "    # Wandb parameters\n",
    "    parser.add_argument('--wandb', action='store_true', default=False)\n",
    "    parser.add_argument('--wandb_project', default='',\n",
    "                        help='project where to wandb log')\n",
    "    parser.add_argument('--wandb_id', default='', type=str,\n",
    "                        help='id of the current run')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "\n",
    "    # Saving Parameters\n",
    "    parser.add_argument('--output_dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    \n",
    "    # parser.add_argument('--mode', type=str, default=\"train\")\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(batch_size=64,\n",
    "    epochs=100,\n",
    "    acum_iter=1,\n",
    "    model='shallow_conv_net',\n",
    "    input_channels=1,\n",
    "    input_electrodes=61,\n",
    "    time_steps=100,\n",
    "    optimizer='sdg', #'adam_w',\n",
    "    criterion='bce',    #####\n",
    "    lr=0.01,\n",
    "    patience=-1,\n",
    "    max_delta=0,\n",
    "    data_path='/vol/aimspace/users/dena/Documents/mae/data/lemon/data_raw_train.pt',\n",
    "    labels_path='/vol/aimspace/users/dena/Documents/ad_benchmarking/ad_benchmarking/data/labels_bin_train.pt',\n",
    "    val_data_path='/vol/aimspace/users/dena/Documents/mae/data/lemon/data_raw_val.pt',\n",
    "    val_labels_path='/vol/aimspace/users/dena/Documents/ad_benchmarking/ad_benchmarking/data/labels_bin_val.pt',\n",
    "    number_samples=64,\n",
    "    num_workers=4,\n",
    "    wandb=False,\n",
    "    wandb_project='',\n",
    "    wandb_id='',\n",
    "    device='cuda',\n",
    "    seed=0,\n",
    "    output_dir='')\n",
    "\n",
    "\n",
    "# Training set size:  1\n",
    "# Validation set size:  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  64\n",
      "Validation set size:  64\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(args.device)\n",
    "\n",
    "# Fix the seed for reproducibility\n",
    "seed = args.seed \n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "dataset_train = dataset.EEGDataset(data_path=args.data_path, labels_path=args.labels_path, \n",
    "                            train=True, number_samples=args.number_samples, length_samples=args.time_steps,\n",
    "                            args=args)\n",
    "dataset_val = dataset.EEGDataset(data_path=args.data_path, labels_path=args.labels_path, \n",
    "                            train=True, number_samples=args.number_samples, length_samples=args.time_steps,\n",
    "                            args=args)\n",
    "\n",
    "print(\"Training set size: \", len(dataset_train))\n",
    "print(\"Validation set size: \", len(dataset_val))\n",
    "\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "sampler_train = torch.utils.data.RandomSampler(dataset_train) \n",
    "\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train, \n",
    "    sampler=sampler_train,\n",
    "    # shuffle=True,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, \n",
    "    sampler=sampler_val,\n",
    "    # shuffle=False,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "model = models.__dict__[args.model](\n",
    "    n_channels=args.input_electrodes, \n",
    "    input_time_length=args.time_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ay1d881v) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch train loss</td><td>▇▇▇█▆▄▆▃▅▄▄▄▄▃▄▄▃▃▃▃▄▃▃▃▃▃▃▂▃▃▂▂▃▃▂▁▃▂▃▁</td></tr><tr><td>batch val loss</td><td>▇▄█▅▇▄▄▄▃▅▃▃▃▃▃▃▃▃▃▃▂▂▂▃▂▂▂▂▂▁▂▂▂▁▂▂▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch train loss</td><td>0.34903</td></tr><tr><td>batch val loss</td><td>0.47987</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pecan-bun-14</strong> at: <a href='https://wandb.ai/adrian_s_playground/ad_benchmarking/runs/ay1d881v' target=\"_blank\">https://wandb.ai/adrian_s_playground/ad_benchmarking/runs/ay1d881v</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240314_161056-ay1d881v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ay1d881v). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/vol/aimspace/users/dena/Documents/ad_benchmarking/ad_benchmarking/wandb/run-20240314_161400-ehdbtbxm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adrian_s_playground/ad_benchmarking/runs/ehdbtbxm' target=\"_blank\">butterscotch-brownie-15</a></strong> to <a href='https://wandb.ai/adrian_s_playground/ad_benchmarking' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adrian_s_playground/ad_benchmarking' target=\"_blank\">https://wandb.ai/adrian_s_playground/ad_benchmarking</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adrian_s_playground/ad_benchmarking/runs/ehdbtbxm' target=\"_blank\">https://wandb.ai/adrian_s_playground/ad_benchmarking/runs/ehdbtbxm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for now: \n",
    "wandb.init(project=args.wandb_project, config=vars(args))\n",
    "data_loader_train = data_loader_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for 100 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss / BCE on 64 train samples: 0.8654236197471619\n",
      "Loss / BCE on 64 val samples: 0.6264200806617737\n",
      "Loss / BCE on 64 train samples: 0.6264022588729858\n",
      "Loss / BCE on 64 val samples: 0.6480107307434082\n",
      "Loss / BCE on 64 train samples: 0.6480274796485901\n",
      "Loss / BCE on 64 val samples: 0.6613976955413818\n",
      "Loss / BCE on 64 train samples: 0.6618583798408508\n",
      "Loss / BCE on 64 val samples: 0.6617220640182495\n",
      "Loss / BCE on 64 train samples: 0.661530077457428\n",
      "Loss / BCE on 64 val samples: 0.6527092456817627\n",
      "Loss / BCE on 64 train samples: 0.6527494192123413\n",
      "Loss / BCE on 64 val samples: 0.6409313678741455\n",
      "Loss / BCE on 64 train samples: 0.6409450173377991\n",
      "Loss / BCE on 64 val samples: 0.6285372972488403\n",
      "Loss / BCE on 64 train samples: 0.628294825553894\n",
      "Loss / BCE on 64 val samples: 0.621527910232544\n",
      "Loss / BCE on 64 train samples: 0.6233018636703491\n",
      "Loss / BCE on 64 val samples: 0.6199936866760254\n",
      "Loss / BCE on 64 train samples: 0.6207655668258667\n",
      "Loss / BCE on 64 val samples: 0.6205404996871948\n",
      "Loss / BCE on 64 train samples: 0.6210224032402039\n",
      "Loss / BCE on 64 val samples: 0.6242982149124146\n",
      "Loss / BCE on 64 train samples: 0.6244984865188599\n",
      "Loss / BCE on 64 val samples: 0.6286550760269165\n",
      "Loss / BCE on 64 train samples: 0.6273753643035889\n",
      "Loss / BCE on 64 val samples: 0.6285039186477661\n",
      "Loss / BCE on 64 train samples: 0.6292576789855957\n",
      "Loss / BCE on 64 val samples: 0.6291289329528809\n",
      "Loss / BCE on 64 train samples: 0.6296185851097107\n",
      "Loss / BCE on 64 val samples: 0.6296310424804688\n",
      "Loss / BCE on 64 train samples: 0.6279473900794983\n",
      "Loss / BCE on 64 val samples: 0.6251938939094543\n",
      "Loss / BCE on 64 train samples: 0.6241021156311035\n",
      "Loss / BCE on 64 val samples: 0.6241751313209534\n",
      "Loss / BCE on 64 train samples: 0.6219217777252197\n",
      "Loss / BCE on 64 val samples: 0.6208968758583069\n",
      "Loss / BCE on 64 train samples: 0.6228982210159302\n",
      "Loss / BCE on 64 val samples: 0.617820143699646\n",
      "Loss / BCE on 64 train samples: 0.6168068051338196\n",
      "Loss / BCE on 64 val samples: 0.6188745498657227\n",
      "Loss / BCE on 64 train samples: 0.6215598583221436\n",
      "Loss / BCE on 64 val samples: 0.6198000907897949\n",
      "Loss / BCE on 64 train samples: 0.6204416155815125\n",
      "Loss / BCE on 64 val samples: 0.621876060962677\n",
      "Loss / BCE on 64 train samples: 0.6181550025939941\n",
      "Loss / BCE on 64 val samples: 0.6170631647109985\n",
      "Loss / BCE on 64 train samples: 0.6183505654335022\n",
      "Loss / BCE on 64 val samples: 0.6170303821563721\n",
      "Loss / BCE on 64 train samples: 0.6259561777114868\n",
      "Loss / BCE on 64 val samples: 0.6211312413215637\n",
      "Loss / BCE on 64 train samples: 0.6221128702163696\n",
      "Loss / BCE on 64 val samples: 0.623529314994812\n",
      "Loss / BCE on 64 train samples: 0.6179105043411255\n",
      "Loss / BCE on 64 val samples: 0.6183789372444153\n",
      "Loss / BCE on 64 train samples: 0.6205833554267883\n",
      "Loss / BCE on 64 val samples: 0.6134186387062073\n",
      "Loss / BCE on 64 train samples: 0.6206186413764954\n",
      "Loss / BCE on 64 val samples: 0.6172118186950684\n",
      "Loss / BCE on 64 train samples: 0.6145048141479492\n",
      "Loss / BCE on 64 val samples: 0.6123676300048828\n",
      "Loss / BCE on 64 train samples: 0.6201602220535278\n",
      "Loss / BCE on 64 val samples: 0.6187945604324341\n",
      "Loss / BCE on 64 train samples: 0.6200608611106873\n",
      "Loss / BCE on 64 val samples: 0.6205873489379883\n",
      "Loss / BCE on 64 train samples: 0.6148374676704407\n",
      "Loss / BCE on 64 val samples: 0.6162444949150085\n",
      "Loss / BCE on 64 train samples: 0.6135827302932739\n",
      "Loss / BCE on 64 val samples: 0.6146587133407593\n",
      "Loss / BCE on 64 train samples: 0.6103723049163818\n",
      "Loss / BCE on 64 val samples: 0.6186635494232178\n",
      "Loss / BCE on 64 train samples: 0.616966962814331\n",
      "Loss / BCE on 64 val samples: 0.6132003664970398\n",
      "Loss / BCE on 64 train samples: 0.6176033616065979\n",
      "Loss / BCE on 64 val samples: 0.6163746118545532\n",
      "Loss / BCE on 64 train samples: 0.6162537336349487\n",
      "Loss / BCE on 64 val samples: 0.617092490196228\n",
      "Loss / BCE on 64 train samples: 0.6141400933265686\n",
      "Loss / BCE on 64 val samples: 0.6129814386367798\n",
      "Loss / BCE on 64 train samples: 0.6115667223930359\n",
      "Loss / BCE on 64 val samples: 0.6142972707748413\n",
      "Loss / BCE on 64 train samples: 0.6133091449737549\n",
      "Loss / BCE on 64 val samples: 0.6129538416862488\n",
      "Loss / BCE on 64 train samples: 0.6219562292098999\n",
      "Loss / BCE on 64 val samples: 0.6119110584259033\n",
      "Loss / BCE on 64 train samples: 0.607484757900238\n",
      "Loss / BCE on 64 val samples: 0.6177762746810913\n",
      "Loss / BCE on 64 train samples: 0.6101464629173279\n",
      "Loss / BCE on 64 val samples: 0.6192982196807861\n",
      "Loss / BCE on 64 train samples: 0.6125367879867554\n",
      "Loss / BCE on 64 val samples: 0.6085779666900635\n",
      "Loss / BCE on 64 train samples: 0.610005259513855\n",
      "Loss / BCE on 64 val samples: 0.6172192096710205\n",
      "Loss / BCE on 64 train samples: 0.6118450164794922\n",
      "Loss / BCE on 64 val samples: 0.60966557264328\n",
      "Loss / BCE on 64 train samples: 0.6139720678329468\n",
      "Loss / BCE on 64 val samples: 0.6107338666915894\n",
      "Loss / BCE on 64 train samples: 0.614618182182312\n",
      "Loss / BCE on 64 val samples: 0.6224396228790283\n",
      "Loss / BCE on 64 train samples: 0.6232927441596985\n",
      "Loss / BCE on 64 val samples: 0.6114879846572876\n",
      "Loss / BCE on 64 train samples: 0.6191451549530029\n",
      "Loss / BCE on 64 val samples: 0.611237108707428\n",
      "Loss / BCE on 64 train samples: 0.6102818250656128\n",
      "Loss / BCE on 64 val samples: 0.6184967756271362\n",
      "Loss / BCE on 64 train samples: 0.6191834211349487\n",
      "Loss / BCE on 64 val samples: 0.6167172789573669\n",
      "Loss / BCE on 64 train samples: 0.6057438254356384\n",
      "Loss / BCE on 64 val samples: 0.6123308539390564\n",
      "Loss / BCE on 64 train samples: 0.616905689239502\n",
      "Loss / BCE on 64 val samples: 0.6150423884391785\n",
      "Loss / BCE on 64 train samples: 0.6117087602615356\n",
      "Loss / BCE on 64 val samples: 0.6076386570930481\n",
      "Loss / BCE on 64 train samples: 0.6119604706764221\n",
      "Loss / BCE on 64 val samples: 0.6134001016616821\n",
      "Loss / BCE on 64 train samples: 0.6057322025299072\n",
      "Loss / BCE on 64 val samples: 0.6102017164230347\n",
      "Loss / BCE on 64 train samples: 0.6099465489387512\n",
      "Loss / BCE on 64 val samples: 0.5998140573501587\n",
      "Loss / BCE on 64 train samples: 0.6057536602020264\n",
      "Loss / BCE on 64 val samples: 0.6190365552902222\n",
      "Loss / BCE on 64 train samples: 0.6075366735458374\n",
      "Loss / BCE on 64 val samples: 0.6070992946624756\n",
      "Loss / BCE on 64 train samples: 0.5980859398841858\n",
      "Loss / BCE on 64 val samples: 0.6168258190155029\n",
      "Loss / BCE on 64 train samples: 0.6062524318695068\n",
      "Loss / BCE on 64 val samples: 0.6145275831222534\n",
      "Loss / BCE on 64 train samples: 0.61232990026474\n",
      "Loss / BCE on 64 val samples: 0.6052083969116211\n",
      "Loss / BCE on 64 train samples: 0.620053768157959\n",
      "Loss / BCE on 64 val samples: 0.6037217378616333\n",
      "Loss / BCE on 64 train samples: 0.6052983999252319\n",
      "Loss / BCE on 64 val samples: 0.6065971255302429\n",
      "Loss / BCE on 64 train samples: 0.6023702621459961\n",
      "Loss / BCE on 64 val samples: 0.6000853180885315\n",
      "Loss / BCE on 64 train samples: 0.6100414991378784\n",
      "Loss / BCE on 64 val samples: 0.6106135845184326\n",
      "Loss / BCE on 64 train samples: 0.5991256237030029\n",
      "Loss / BCE on 64 val samples: 0.6033761501312256\n",
      "Loss / BCE on 64 train samples: 0.6046128273010254\n",
      "Loss / BCE on 64 val samples: 0.6000906825065613\n",
      "Loss / BCE on 64 train samples: 0.6010873317718506\n",
      "Loss / BCE on 64 val samples: 0.6013984680175781\n",
      "Loss / BCE on 64 train samples: 0.5987421274185181\n",
      "Loss / BCE on 64 val samples: 0.592536449432373\n",
      "Loss / BCE on 64 train samples: 0.6014776229858398\n",
      "Loss / BCE on 64 val samples: 0.6097062826156616\n",
      "Loss / BCE on 64 train samples: 0.6117457151412964\n",
      "Loss / BCE on 64 val samples: 0.5987652540206909\n",
      "Loss / BCE on 64 train samples: 0.5976513624191284\n",
      "Loss / BCE on 64 val samples: 0.59331214427948\n",
      "Loss / BCE on 64 train samples: 0.6071817874908447\n",
      "Loss / BCE on 64 val samples: 0.6122123003005981\n",
      "Loss / BCE on 64 train samples: 0.5950202941894531\n",
      "Loss / BCE on 64 val samples: 0.594745397567749\n",
      "Loss / BCE on 64 train samples: 0.6096823811531067\n",
      "Loss / BCE on 64 val samples: 0.5989850163459778\n",
      "Loss / BCE on 64 train samples: 0.6001293659210205\n",
      "Loss / BCE on 64 val samples: 0.5995161533355713\n",
      "Loss / BCE on 64 train samples: 0.5921639204025269\n",
      "Loss / BCE on 64 val samples: 0.5927225947380066\n",
      "Loss / BCE on 64 train samples: 0.5970126986503601\n",
      "Loss / BCE on 64 val samples: 0.6144731044769287\n",
      "Loss / BCE on 64 train samples: 0.5961518287658691\n",
      "Loss / BCE on 64 val samples: 0.6059303283691406\n",
      "Loss / BCE on 64 train samples: 0.6068434119224548\n",
      "Loss / BCE on 64 val samples: 0.6029119491577148\n",
      "Loss / BCE on 64 train samples: 0.601440966129303\n",
      "Loss / BCE on 64 val samples: 0.6072558760643005\n",
      "Loss / BCE on 64 train samples: 0.6140627861022949\n",
      "Loss / BCE on 64 val samples: 0.6216703653335571\n",
      "Loss / BCE on 64 train samples: 0.5743387937545776\n",
      "Loss / BCE on 64 val samples: 0.588753342628479\n",
      "Loss / BCE on 64 train samples: 0.598746657371521\n",
      "Loss / BCE on 64 val samples: 0.5734035968780518\n",
      "Loss / BCE on 64 train samples: 0.6039983630180359\n",
      "Loss / BCE on 64 val samples: 0.5870767831802368\n",
      "Loss / BCE on 64 train samples: 0.5775315761566162\n",
      "Loss / BCE on 64 val samples: 0.586432933807373\n",
      "Loss / BCE on 64 train samples: 0.5918351411819458\n",
      "Loss / BCE on 64 val samples: 0.5953956842422485\n",
      "Loss / BCE on 64 train samples: 0.6243411898612976\n",
      "Loss / BCE on 64 val samples: 0.6038168668746948\n",
      "Loss / BCE on 64 train samples: 0.6099313497543335\n",
      "Loss / BCE on 64 val samples: 0.5952881574630737\n",
      "Loss / BCE on 64 train samples: 0.6075785160064697\n",
      "Loss / BCE on 64 val samples: 0.6002753376960754\n",
      "Loss / BCE on 64 train samples: 0.5907201766967773\n",
      "Loss / BCE on 64 val samples: 0.5716159343719482\n",
      "Loss / BCE on 64 train samples: 0.613761842250824\n",
      "Loss / BCE on 64 val samples: 0.5842698812484741\n",
      "Loss / BCE on 64 train samples: 0.587309718132019\n",
      "Loss / BCE on 64 val samples: 0.5861794948577881\n",
      "Loss / BCE on 64 train samples: 0.5939791202545166\n",
      "Loss / BCE on 64 val samples: 0.6069256663322449\n",
      "Loss / BCE on 64 train samples: 0.607826828956604\n",
      "Loss / BCE on 64 val samples: 0.5823591947555542\n",
      "Loss / BCE on 64 train samples: 0.5871410965919495\n",
      "Loss / BCE on 64 val samples: 0.5983624458312988\n",
      "Loss / BCE on 64 train samples: 0.5818958282470703\n",
      "Loss / BCE on 64 val samples: 0.6094446182250977\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "# eval_criterion = \"bce\"\n",
    "# criterion = nn.BCELoss()  # !!!! \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.95))\n",
    "\n",
    "# Define callbacks\n",
    "early_stop = EarlyStop(patience=args.patience, max_delta=args.max_delta)\n",
    "\n",
    "print(f\"Start training for {args.epochs} epochs\")\n",
    "\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    mean_loss_epoch_train = train_one_epoch(model, data_loader_train, optimizer, device, epoch, args=args) #loss_scaler, criterion\n",
    "    print(f\"Loss / BCE on {len(dataset_train)} train samples: {mean_loss_epoch_train}\")\n",
    "\n",
    "    mean_loss_epoch_val = evaluate(model, data_loader_val, device, epoch, args=args) \n",
    "    print(f\"Loss / BCE on {len(dataset_val)} val samples: {mean_loss_epoch_val}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mae2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
