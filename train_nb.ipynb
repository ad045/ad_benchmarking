{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madrian-dendorfer\u001b[0m (\u001b[33madrian_s_playground\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import argparse \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import json as json\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "import models as models\n",
    "\n",
    "import wandb\n",
    "# from os import Path\n",
    "\n",
    "import models \n",
    "import datasets\n",
    "import dataset\n",
    "\n",
    "import numpy as np\n",
    "import time as time \n",
    "import util.misc as misc\n",
    "# from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "from util.callbacks import EarlyStop\n",
    "\n",
    "from util.engine_train import train_one_epoch, evaluate # evaluate_online\n",
    "\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser(\"NN training\")\n",
    "\n",
    "    parser.add_argument('--batch_size', default=64, type=int)\n",
    "    parser.add_argument('--epochs', default=400, type=int)\n",
    "    parser.add_argument('--acum_iter', default=1, type=int) \n",
    "\n",
    "    parser.add_argument('--model', default='shallow_conv_net', type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "    \n",
    "    # Model parameters\n",
    "    parser.add_argument('--input_channels', type=int, default=1, metavar='N',\n",
    "                        help='input channels')\n",
    "    parser.add_argument('--input_electrodes', type=int, default=61, metavar='N',\n",
    "                        help='input electrodes')\n",
    "    parser.add_argument('--time_steps', type=int, default=100, metavar='N',\n",
    "                        help='input length')\n",
    "    # parser.add_argument('--length_samples', default=200, \n",
    "    #                     help='length of samples') \n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument('--optimizer', type=str, default=\"adam_w\", \n",
    "                        help='optimizer type') \n",
    "    parser.add_argument('--lr', type=float, default=1e-3, metavar='LR',\n",
    "                        help='learning rate') \n",
    "\n",
    "    # Callback parameters\n",
    "    parser.add_argument('--patience', default=-1, type=float,\n",
    "                        help='Early stopping whether val is worse than train for specified nb of epochs (default: -1, i.e. no early stopping)')\n",
    "    parser.add_argument('--max_delta', default=0, type=float,\n",
    "                        help='Early stopping threshold (val has to be worse than (train+delta)) (default: 0)')\n",
    "\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument('--data_path', \n",
    "                        # default='_.pt',\n",
    "                        default=\"/vol/aimspace/users/dena/Documents/mae/data/lemon/data_raw_train.pt\",\n",
    "                        type=str,\n",
    "                        help='train dataset path')\n",
    "\n",
    "    parser.add_argument('--labels_path', \n",
    "                        # default='_.pt', \n",
    "                        default=\"/vol/aimspace/users/dena/Documents/ad_benchmarking/ad_benchmarking/data/labels_bin_train.pt\", #labels_raw_train.pt\",\n",
    "                        type=str,\n",
    "                        help='train labels path')\n",
    "    parser.add_argument('--val_data_path', \n",
    "                        # default='', \n",
    "                        default=\"/vol/aimspace/users/dena/Documents/mae/data/lemon/data_raw_val.pt\",\n",
    "                        type=str,\n",
    "                        help='validation dataset path')\n",
    "    parser.add_argument('--val_labels_path', \n",
    "                        # default='_.pt', \n",
    "                        default=\"/vol/aimspace/users/dena/Documents/ad_benchmarking/ad_benchmarking/data/labels_bin_val.pt\", # \"labels_raw_val.pt\"\n",
    "                        type=str,\n",
    "                        help='validation labels path')\n",
    "    parser.add_argument('--number_samples', default=1, type=int, # | str, \n",
    "                        help='number of samples on which network should train on. \"None\" means all samples.')\n",
    "    \n",
    "    \n",
    "    # Wandb parameters\n",
    "    parser.add_argument('--wandb', action='store_true', default=False)\n",
    "    parser.add_argument('--wandb_project', default='',\n",
    "                        help='project where to wandb log')\n",
    "    parser.add_argument('--wandb_id', default='', type=str,\n",
    "                        help='id of the current run')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "\n",
    "    # Saving Parameters\n",
    "    parser.add_argument('--output_dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    \n",
    "    # parser.add_argument('--mode', type=str, default=\"train\")\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(batch_size=16,\n",
    "    epochs=100,\n",
    "    acum_iter=1,\n",
    "    model='deep_conv_net', #first_shallow_conv_net_regression', #shallow_conv_net',  # deep_conv_net, simple_classifier\n",
    "    input_channels=1,\n",
    "    input_electrodes=61,\n",
    "    time_steps=210,\n",
    "    optimizer='adamw', #'adam_w',\n",
    "    criterion='mse',   \n",
    "    lr=0.1,\n",
    "    patience=1000,\n",
    "    sufficient_accuracy=1, #-np.inf, \n",
    "    max_delta=0,\n",
    "    data_path='/vol/aimspace/users/dena/Documents/mae/data/lemon/data_raw_train.pt',\n",
    "    # Classification\n",
    "    # labels_path='/vol/aimspace/users/dena/Documents/ad_benchmarking/ad_benchmarking/data/labels_bin_train.pt',\n",
    "    #Regression\n",
    "    labels_path='/u/home/dena/Documents/mae/data/lemon/labels_raw_train.pt',\n",
    "    val_data_path='/vol/aimspace/users/dena/Documents/mae/data/lemon/data_raw_val.pt',\n",
    "    val_labels_path='/vol/aimspace/users/dena/Documents/ad_benchmarking/ad_benchmarking/data/labels_bin_val.pt',\n",
    "    number_samples=1, #16, #64,\n",
    "    num_workers=4,\n",
    "    wandb=False,\n",
    "    wandb_project='',\n",
    "    wandb_id='',\n",
    "    device='cpu', #cuda',\n",
    "    seed=0,\n",
    "    output_dir='')\n",
    "\n",
    "\n",
    "# Training set size:  1\n",
    "# Validation set size:  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6250],\n",
       "        [0.2250],\n",
       "        [0.2250],\n",
       "        [0.3250],\n",
       "        [0.2750],\n",
       "        [0.2250],\n",
       "        [0.6250],\n",
       "        [0.2250],\n",
       "        [0.2750],\n",
       "        [0.6750],\n",
       "        [0.2250],\n",
       "        [0.2250],\n",
       "        [0.2750],\n",
       "        [0.7250],\n",
       "        [0.2250],\n",
       "        [0.7250],\n",
       "        [0.6750],\n",
       "        [0.2250],\n",
       "        [0.2750],\n",
       "        [0.2250],\n",
       "        [0.6750],\n",
       "        [0.2250],\n",
       "        [0.2750],\n",
       "        [0.2750],\n",
       "        [0.2250],\n",
       "        [0.2250],\n",
       "        [0.2750],\n",
       "        [0.2250],\n",
       "        [0.2750],\n",
       "        [0.6250],\n",
       "        [0.2250],\n",
       "        [0.2250],\n",
       "        [0.7250],\n",
       "        [0.2250],\n",
       "        [0.3250],\n",
       "        [0.2250],\n",
       "        [0.6250],\n",
       "        [0.2250],\n",
       "        [0.2250],\n",
       "        [0.2750],\n",
       "        [0.2250],\n",
       "        [0.6750],\n",
       "        [0.2750],\n",
       "        [0.6250],\n",
       "        [0.2250],\n",
       "        [0.2750],\n",
       "        [0.2250],\n",
       "        [0.7250],\n",
       "        [0.2750],\n",
       "        [0.6750],\n",
       "        [0.7750],\n",
       "        [0.2750],\n",
       "        [0.2250],\n",
       "        [0.7250],\n",
       "        [0.2250],\n",
       "        [0.6250],\n",
       "        [0.2750],\n",
       "        [0.2250],\n",
       "        [0.2750],\n",
       "        [0.6750],\n",
       "        [0.7750],\n",
       "        [0.7250],\n",
       "        [0.2250],\n",
       "        [0.2250],\n",
       "        [0.3250],\n",
       "        [0.2250],\n",
       "        [0.2750],\n",
       "        [0.7250],\n",
       "        [0.2750],\n",
       "        [0.6250],\n",
       "        [0.2750],\n",
       "        [0.6750],\n",
       "        [0.3750],\n",
       "        [0.7250],\n",
       "        [0.2750],\n",
       "        [0.2750],\n",
       "        [0.2750],\n",
       "        [0.2250],\n",
       "        [0.2750],\n",
       "        [0.2250],\n",
       "        [0.6750],\n",
       "        [0.2750],\n",
       "        [0.3250],\n",
       "        [0.2750],\n",
       "        [0.6750],\n",
       "        [0.2250],\n",
       "        [0.2250],\n",
       "        [0.5750],\n",
       "        [0.2750],\n",
       "        [0.6750],\n",
       "        [0.2250],\n",
       "        [0.7250],\n",
       "        [0.2750],\n",
       "        [0.2250],\n",
       "        [0.6250],\n",
       "        [0.6750],\n",
       "        [0.2250],\n",
       "        [0.6250],\n",
       "        [0.6750],\n",
       "        [0.7250],\n",
       "        [0.7250],\n",
       "        [0.3250],\n",
       "        [0.2750],\n",
       "        [0.3250],\n",
       "        [0.2250],\n",
       "        [0.2250],\n",
       "        [0.2750],\n",
       "        [0.3250],\n",
       "        [0.7750],\n",
       "        [0.2250],\n",
       "        [0.2250],\n",
       "        [0.2250],\n",
       "        [0.2250],\n",
       "        [0.2250],\n",
       "        [0.2250],\n",
       "        [0.2250],\n",
       "        [0.2250],\n",
       "        [0.2250],\n",
       "        [0.2750],\n",
       "        [0.6750],\n",
       "        [0.2750],\n",
       "        [0.7250],\n",
       "        [0.2250],\n",
       "        [0.2250],\n",
       "        [0.2750],\n",
       "        [0.2750],\n",
       "        [0.5750],\n",
       "        [0.6250],\n",
       "        [0.2750],\n",
       "        [0.2250],\n",
       "        [0.6750],\n",
       "        [0.2250],\n",
       "        [0.2750],\n",
       "        [0.2750],\n",
       "        [0.2250],\n",
       "        [0.6750],\n",
       "        [0.2750],\n",
       "        [0.7250],\n",
       "        [0.2250],\n",
       "        [0.2750],\n",
       "        [0.7250],\n",
       "        [0.2750],\n",
       "        [0.2250],\n",
       "        [0.6250],\n",
       "        [0.2250],\n",
       "        [0.2250],\n",
       "        [0.2750],\n",
       "        [0.6750],\n",
       "        [0.6750],\n",
       "        [0.2750]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.load(args.labels_path, map_location=torch.device('cpu')) # load to ram\n",
    "X/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  1\n",
      "Validation set size:  1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:zhm23x6d) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>guessed age</td><td>███████████████████▇▇▇▇▇▇▇▇▇▆▅▄▂▁▁▂▅▇███</td></tr><tr><td>mean train MAE loss</td><td>▁▂▁▂▄▂▂▄▁▂▃▅▂▃▂▅▅▃▃▃▄▄▆▃▅█▁▃▅▄▁▄▃▅▃▃▃▁▄▁</td></tr><tr><td>mean val MAE loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▄▅▇██▇▄▁▁▁▁</td></tr><tr><td>true age</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>guessed age</td><td>41.79777</td></tr><tr><td>mean train MAE loss</td><td>67.95949</td></tr><tr><td>mean val MAE loss</td><td>41.17277</td></tr><tr><td>true age</td><td>0.625</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wobbly-bush-77</strong> at: <a href='https://wandb.ai/adrian_s_playground/ad_benchmarking/runs/zhm23x6d' target=\"_blank\">https://wandb.ai/adrian_s_playground/ad_benchmarking/runs/zhm23x6d</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240318_155231-zhm23x6d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:zhm23x6d). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/vol/aimspace/users/dena/Documents/ad_benchmarking/ad_benchmarking/wandb/run-20240318_155631-rluup11c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adrian_s_playground/ad_benchmarking/runs/rluup11c' target=\"_blank\">olive-eon-78</a></strong> to <a href='https://wandb.ai/adrian_s_playground/ad_benchmarking' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adrian_s_playground/ad_benchmarking' target=\"_blank\">https://wandb.ai/adrian_s_playground/ad_benchmarking</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adrian_s_playground/ad_benchmarking/runs/rluup11c' target=\"_blank\">https://wandb.ai/adrian_s_playground/ad_benchmarking/runs/rluup11c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print('job dir: {}'.format(os.path.dirname(os.path.realpath(__file__))))\n",
    "# print(\"{}\".format(args).replace(', ', ',\\n'))\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "# Fix the seed for reproducibility\n",
    "seed = args.seed \n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "dataset_train = dataset.EEGDataset(data_path=args.data_path, labels_path=args.labels_path, \n",
    "                            train=True, number_samples=args.number_samples, length_samples=args.time_steps, scaled=True,\n",
    "                            args=args)\n",
    "dataset_val = dataset.EEGDataset(data_path=args.data_path, labels_path=args.labels_path, \n",
    "                            train=True, number_samples=args.number_samples, length_samples=args.time_steps, scaled=True,\n",
    "                            args=args)\n",
    "\n",
    "print(\"Training set size: \", len(dataset_train))\n",
    "print(\"Validation set size: \", len(dataset_val))\n",
    "\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "sampler_train = torch.utils.data.RandomSampler(dataset_train) \n",
    "\n",
    "# # wandb logging\n",
    "# if args.wandb == True:\n",
    "#     config = vars(args)\n",
    "#     if args.wandb_id:\n",
    "#         wandb.init(project=args.wandb_project, id=args.wandb_id, config=config)\n",
    "#     else:\n",
    "#         wandb.init(project=args.wandb_project, config=config)\n",
    "wandb.init(project=args.wandb_project, config=vars(args))\n",
    "\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train, \n",
    "    sampler=sampler_train,\n",
    "    # shuffle=True,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    "    # pin_memory=args.pin_mem,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, \n",
    "    sampler=sampler_val,\n",
    "    # shuffle=False,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    "    # pin_memory=args.pin_mem,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "model = models.__dict__[args.model](\n",
    "    n_channels=args.input_electrodes, \n",
    "    input_time_length=args.time_steps, \n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# eval_criterion = \"bce\"\n",
    "if args.criterion == \"bce\": \n",
    "    criterion = torch.nn.BCELoss() # For classification\n",
    "\n",
    "elif args.criterion == \"mae\": \n",
    "    criterion = torch.nn.L1Loss() # For regression \n",
    "\n",
    "elif args.criterion == \"mse\": \n",
    "    criterion = torch.nn.MSELoss() # For regression \n",
    "\n",
    "\n",
    "\n",
    "if args.optimizer == \"sgd\":\n",
    "    optimizer = optim.SGD(model.parameters(),\n",
    "                            lr=args.lr, momentum=0.9)\n",
    "elif args.optimizer == \"adam\":\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                            lr=args.lr)\n",
    "elif args.optimizer == \"adamw\": \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.95))\n",
    "\n",
    "else: \n",
    "    print(\"Attention: No optimier chosen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now: \n",
    "data_loader_train = data_loader_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFICATION\n",
    "\n",
    "# # Define callbacks\n",
    "# # early_stop = EarlyStop(patience=args.patience, max_delta=args.max_delta)\n",
    "\n",
    "# print(f\"Start training for {args.epochs} epochs\")\n",
    "\n",
    "# min_val_metric = np.inf\n",
    "# counter = 0 \n",
    "\n",
    "# for epoch in range(args.epochs): \n",
    "    \n",
    "#     mean_loss_epoch_train_bce, mean_loss_epoch_train_L1 = train_one_epoch(model, data_loader_train, optimizer, criterion, device, epoch, args=args) #loss_scaler, criterion\n",
    "#     print(f\"Loss / BCE on {len(dataset_train)} train samples: {mean_loss_epoch_train_bce}\")\n",
    "\n",
    "#     # mean_loss_epoch_val_bce, mean_loss_epoch_val_L1 = evaluate(model, data_loader_val, criterion, device, epoch, args=args) \n",
    "#     target, output, mean_loss_epoch_val_bce, mean_loss_epoch_val_L1 = evaluate(model, data_loader_val, criterion, device, epoch, args=args) \n",
    "#     print(target, output) \n",
    "#     print(f\"Loss / BCE on {len(dataset_val)} val samples BCE: {mean_loss_epoch_val_bce}, val samples MAE: {mean_loss_epoch_val_L1}\")\n",
    "#     wandb.log({\"mean train BCE loss\": mean_loss_epoch_train_bce,\n",
    "#                \"mean train MAE loss\": mean_loss_epoch_train_L1, \n",
    "#                \"mean val BCE loss\": mean_loss_epoch_val_bce, \n",
    "#                \"mean val MAE loss\": mean_loss_epoch_val_L1, \n",
    "#                \"epoch\": epoch})\n",
    "    \n",
    "#     # Early Stopping\n",
    "#     print(f\"Sufficient accuracy: {args.sufficient_accuracy}.\")\n",
    "#     print(f\"patience: {args.patience > -1}.\")\n",
    "#     print(f\"stuff: {mean_loss_epoch_train_L1 < args.sufficient_accuracy}.\")\n",
    "#     if args.patience > -1: \n",
    "#         if mean_loss_epoch_train_L1 < args.sufficient_accuracy: \n",
    "#             break\n",
    "#         elif mean_loss_epoch_train_L1 < min_val_metric: \n",
    "#             min_val_metric = mean_loss_epoch_train_L1\n",
    "#             counter == 0\n",
    "#         elif mean_loss_epoch_train_L1 > min_val_metric: \n",
    "#             counter += 1\n",
    "#             if counter > args.patience:\n",
    "#                 print(f\"stopped early at epoch {epoch}.\")\n",
    "#                 break \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for 100 epochs\n",
      "Loss / MAE on 1 train samples: 10.05834329163657\n",
      "tensor([[0.6250]]) tensor([[88831.4141]])\n",
      "Loss / MAE on 1 val samples: 88830.78961711418\n",
      "Loss / MAE on 1 train samples: 18328.677794306386\n",
      "tensor([[0.6250]]) tensor([[344435.1875]])\n",
      "Loss / MAE on 1 val samples: 344434.56235401233\n",
      "Loss / MAE on 1 train samples: 4698.5354475158\n",
      "tensor([[0.6250]]) tensor([[269748.7188]])\n",
      "Loss / MAE on 1 val samples: 269748.0879932238\n",
      "Loss / MAE on 1 train samples: 34165.12407594915\n",
      "tensor([[0.6250]]) tensor([[10420.3057]])\n",
      "Loss / MAE on 1 val samples: 10419.680609308522\n",
      "Loss / MAE on 1 train samples: 73474.1068336322\n",
      "tensor([[0.6250]]) tensor([[-4049.9871]])\n",
      "Loss / MAE on 1 val samples: 4050.6120525174956\n",
      "Loss / MAE on 1 train samples: 39244.4542260432\n",
      "tensor([[0.6250]]) tensor([[-2126.1521]])\n",
      "Loss / MAE on 1 val samples: 2126.7771392414393\n",
      "Loss / MAE on 1 train samples: 38316.818679921744\n",
      "tensor([[0.6250]]) tensor([[-5792.8364]])\n",
      "Loss / MAE on 1 val samples: 5793.461486883295\n",
      "Loss / MAE on 1 train samples: 17122.321870879547\n",
      "tensor([[0.6250]]) tensor([[-30439.6406]])\n",
      "Loss / MAE on 1 val samples: 30440.265964672515\n",
      "Loss / MAE on 1 train samples: 54048.43892287732\n",
      "tensor([[0.6250]]) tensor([[-33858.6328]])\n",
      "Loss / MAE on 1 val samples: 33859.256932189164\n",
      "Loss / MAE on 1 train samples: 79522.9487317969\n",
      "tensor([[0.6250]]) tensor([[-36621.7031]])\n",
      "Loss / MAE on 1 val samples: 36622.327615813825\n",
      "Loss / MAE on 1 train samples: 26560.36314934342\n",
      "tensor([[0.6250]]) tensor([[-61087.0742]])\n",
      "Loss / MAE on 1 val samples: 61087.6982706011\n",
      "Loss / MAE on 1 train samples: 24430.386592571555\n",
      "tensor([[0.6250]]) tensor([[-61997.6016]])\n",
      "Loss / MAE on 1 val samples: 61998.2265552814\n",
      "Loss / MAE on 1 train samples: 42369.30824606416\n",
      "tensor([[0.6250]]) tensor([[-45495.8672]])\n",
      "Loss / MAE on 1 val samples: 45496.49164496093\n",
      "Loss / MAE on 1 train samples: 25204.713410888053\n",
      "tensor([[0.6250]]) tensor([[-28278.0918]])\n",
      "Loss / MAE on 1 val samples: 28278.716802570798\n",
      "Loss / MAE on 1 train samples: 3349.0950999159836\n",
      "tensor([[0.6250]]) tensor([[-18097.3730]])\n",
      "Loss / MAE on 1 val samples: 18097.99812133928\n",
      "Loss / MAE on 1 train samples: 19551.029984057106\n",
      "tensor([[0.6250]]) tensor([[-10603.5586]])\n",
      "Loss / MAE on 1 val samples: 10604.183702671318\n",
      "Loss / MAE on 1 train samples: 65287.505983151175\n",
      "tensor([[0.6250]]) tensor([[-1011.7904]])\n",
      "Loss / MAE on 1 val samples: 1012.4153976999758\n",
      "Loss / MAE on 1 train samples: 10278.599582384752\n",
      "tensor([[0.6250]]) tensor([[-2560.6833]])\n",
      "Loss / MAE on 1 val samples: 2561.3083570706594\n",
      "Loss / MAE on 1 train samples: 56851.800213361756\n",
      "tensor([[0.6250]]) tensor([[-1875.2354]])\n",
      "Loss / MAE on 1 val samples: 1875.8603359525464\n",
      "Loss / MAE on 1 train samples: 80446.83415150655\n",
      "tensor([[0.6250]]) tensor([[-1393.6304]])\n",
      "Loss / MAE on 1 val samples: 1394.255356812374\n",
      "Loss / MAE on 1 train samples: 15547.46826757495\n",
      "tensor([[0.6250]]) tensor([[-812.8578]])\n",
      "Loss / MAE on 1 val samples: 813.4828670599031\n",
      "Loss / MAE on 1 train samples: 37357.34323730744\n",
      "tensor([[0.6250]]) tensor([[-444.1012]])\n",
      "Loss / MAE on 1 val samples: 444.72623250152446\n",
      "Loss / MAE on 1 train samples: 23798.513773185503\n",
      "tensor([[0.6250]]) tensor([[-737.2278]])\n",
      "Loss / MAE on 1 val samples: 737.8528393250242\n",
      "Loss / MAE on 1 train samples: 34406.41519033914\n",
      "tensor([[0.6250]]) tensor([[-833.7245]])\n",
      "Loss / MAE on 1 val samples: 834.3495595372482\n",
      "Loss / MAE on 1 train samples: 15864.075965841817\n",
      "tensor([[0.6250]]) tensor([[-1170.6128]])\n",
      "Loss / MAE on 1 val samples: 1171.237806766841\n",
      "Loss / MAE on 1 train samples: 28607.897456384315\n",
      "tensor([[0.6250]]) tensor([[-4299.0078]])\n",
      "Loss / MAE on 1 val samples: 4299.632775016955\n",
      "Loss / MAE on 1 train samples: 9680.730497681981\n",
      "tensor([[0.6250]]) tensor([[-6385.1313]])\n",
      "Loss / MAE on 1 val samples: 6385.756337349555\n",
      "Loss / MAE on 1 train samples: 31569.33960546213\n",
      "tensor([[0.6250]]) tensor([[-8437.8301]])\n",
      "Loss / MAE on 1 val samples: 8438.45530888207\n",
      "Loss / MAE on 1 train samples: 32724.074900980777\n",
      "tensor([[0.6250]]) tensor([[-11755.1445]])\n",
      "Loss / MAE on 1 val samples: 11755.76930702538\n",
      "Loss / MAE on 1 train samples: 31650.323359327627\n",
      "tensor([[0.6250]]) tensor([[-12667.5059]])\n",
      "Loss / MAE on 1 val samples: 12668.130722407312\n",
      "Loss / MAE on 1 train samples: 11089.27594640989\n",
      "tensor([[0.6250]]) tensor([[-13642.6357]])\n",
      "Loss / MAE on 1 val samples: 13643.260607347498\n",
      "Loss / MAE on 1 train samples: 27691.71384412673\n",
      "tensor([[0.6250]]) tensor([[-13323.9678]])\n",
      "Loss / MAE on 1 val samples: 13324.59260165203\n",
      "Loss / MAE on 1 train samples: 60834.709459321006\n",
      "tensor([[0.6250]]) tensor([[-22066.6309]])\n",
      "Loss / MAE on 1 val samples: 22067.255742388992\n",
      "Loss / MAE on 1 train samples: 54147.925283430755\n",
      "tensor([[0.6250]]) tensor([[-28585.4512]])\n",
      "Loss / MAE on 1 val samples: 28586.076610825767\n",
      "Loss / MAE on 1 train samples: 57641.17451960881\n",
      "tensor([[0.6250]]) tensor([[-28209.4531]])\n",
      "Loss / MAE on 1 val samples: 28210.07819911175\n",
      "Loss / MAE on 1 train samples: 5130.665932425951\n",
      "tensor([[0.6250]]) tensor([[-20796.1328]])\n",
      "Loss / MAE on 1 val samples: 20796.757439562545\n",
      "Loss / MAE on 1 train samples: 32758.90861658856\n",
      "tensor([[0.6250]]) tensor([[-3164.2166]])\n",
      "Loss / MAE on 1 val samples: 3164.8415442167084\n",
      "Loss / MAE on 1 train samples: 8748.24982496499\n",
      "tensor([[0.6250]]) tensor([[25178.3965]])\n",
      "Loss / MAE on 1 val samples: 25177.771783857283\n",
      "Loss / MAE on 1 train samples: 61485.34632333138\n",
      "tensor([[0.6250]]) tensor([[33138.2773]])\n",
      "Loss / MAE on 1 val samples: 33137.65326633737\n",
      "Loss / MAE on 1 train samples: 65732.21479989853\n",
      "tensor([[0.6250]]) tensor([[23922.6035]])\n",
      "Loss / MAE on 1 val samples: 23921.978513492566\n",
      "Loss / MAE on 1 train samples: 72712.79117184266\n",
      "tensor([[0.6250]]) tensor([[-2165.8835]])\n",
      "Loss / MAE on 1 val samples: 2166.5085967980835\n",
      "Loss / MAE on 1 train samples: 16665.371564264628\n",
      "tensor([[0.6250]]) tensor([[-29297.6660]])\n",
      "Loss / MAE on 1 val samples: 29298.291554286916\n",
      "Loss / MAE on 1 train samples: 15776.182354660457\n",
      "tensor([[0.6250]]) tensor([[-57190.6953]])\n",
      "Loss / MAE on 1 val samples: 57191.321299651754\n",
      "Loss / MAE on 1 train samples: 7599.318624369391\n",
      "tensor([[0.6250]]) tensor([[-90164.3047]])\n",
      "Loss / MAE on 1 val samples: 90164.93047743119\n",
      "Loss / MAE on 1 train samples: 21234.942275351255\n",
      "tensor([[0.6250]]) tensor([[-114910.6953]])\n",
      "Loss / MAE on 1 val samples: 114911.31829371727\n",
      "Loss / MAE on 1 train samples: 87888.62981068713\n",
      "tensor([[0.6250]]) tensor([[-88066.2109]])\n",
      "Loss / MAE on 1 val samples: 88066.83716359979\n",
      "Loss / MAE on 1 train samples: 66814.10545311522\n",
      "tensor([[0.6250]]) tensor([[-50792.1367]])\n",
      "Loss / MAE on 1 val samples: 50792.76294906588\n",
      "Loss / MAE on 1 train samples: 4713.988180968689\n",
      "tensor([[0.6250]]) tensor([[-24611.7383]])\n",
      "Loss / MAE on 1 val samples: 24612.3637223246\n",
      "Loss / MAE on 1 train samples: 4333.506356722002\n",
      "tensor([[0.6250]]) tensor([[-7553.3511]])\n",
      "Loss / MAE on 1 val samples: 7553.976171527152\n",
      "Loss / MAE on 1 train samples: 80776.60552412437\n",
      "tensor([[0.6250]]) tensor([[-10050.2393]])\n",
      "Loss / MAE on 1 val samples: 10050.864241447101\n",
      "Loss / MAE on 1 train samples: 54421.31360965113\n",
      "tensor([[0.6250]]) tensor([[-16959.0879]])\n",
      "Loss / MAE on 1 val samples: 16959.71320512231\n",
      "Loss / MAE on 1 train samples: 22879.739092426295\n",
      "tensor([[0.6250]]) tensor([[-23015.8613]])\n",
      "Loss / MAE on 1 val samples: 23016.48661286079\n",
      "Loss / MAE on 1 train samples: 7062.709828188292\n",
      "tensor([[0.6250]]) tensor([[-27031.8594]])\n",
      "Loss / MAE on 1 val samples: 27032.48416257744\n",
      "Loss / MAE on 1 train samples: 15126.893605636618\n",
      "tensor([[0.6250]]) tensor([[-30241.0449]])\n",
      "Loss / MAE on 1 val samples: 30241.66979516839\n",
      "Loss / MAE on 1 train samples: 2553.352336497428\n",
      "tensor([[0.6250]]) tensor([[-31037.0996]])\n",
      "Loss / MAE on 1 val samples: 31037.72465887279\n",
      "Loss / MAE on 1 train samples: 4152.98494285097\n",
      "tensor([[0.6250]]) tensor([[-29882.1289]])\n",
      "Loss / MAE on 1 val samples: 29882.754357655856\n",
      "Loss / MAE on 1 train samples: 2337.8929419892443\n",
      "tensor([[0.6250]]) tensor([[-29271.1562]])\n",
      "Loss / MAE on 1 val samples: 29271.781360211065\n",
      "Loss / MAE on 1 train samples: 22622.023284843468\n",
      "tensor([[0.6250]]) tensor([[-27434.0938]])\n",
      "Loss / MAE on 1 val samples: 27434.7190253518\n",
      "Loss / MAE on 1 train samples: 40230.24554051342\n",
      "tensor([[0.6250]]) tensor([[-22824.7539]])\n",
      "Loss / MAE on 1 val samples: 22825.378857753927\n",
      "Loss / MAE on 1 train samples: 26735.91919123036\n",
      "tensor([[0.6250]]) tensor([[-17707.8379]])\n",
      "Loss / MAE on 1 val samples: 17708.463061485603\n",
      "Loss / MAE on 1 train samples: 6366.915830093877\n",
      "tensor([[0.6250]]) tensor([[-15053.1230]])\n",
      "Loss / MAE on 1 val samples: 15053.747971850731\n",
      "Loss / MAE on 1 train samples: 12210.51392819422\n",
      "tensor([[0.6250]]) tensor([[-13420.9590]])\n",
      "Loss / MAE on 1 val samples: 13421.583811160292\n",
      "Loss / MAE on 1 train samples: 3379.2278568044394\n",
      "tensor([[0.6250]]) tensor([[-11107.7510]])\n",
      "Loss / MAE on 1 val samples: 11108.37593890304\n",
      "Loss / MAE on 1 train samples: 41413.26968798769\n",
      "tensor([[0.6250]]) tensor([[-10004.4414]])\n",
      "Loss / MAE on 1 val samples: 10005.066316621795\n",
      "Loss / MAE on 1 train samples: 21170.5047988351\n",
      "tensor([[0.6250]]) tensor([[-9442.0127]])\n",
      "Loss / MAE on 1 val samples: 9442.637767064878\n",
      "Loss / MAE on 1 train samples: 33680.917724284176\n",
      "tensor([[0.6250]]) tensor([[-9548.7334]])\n",
      "Loss / MAE on 1 val samples: 9549.358512486584\n",
      "Loss / MAE on 1 train samples: 30015.890322877316\n",
      "tensor([[0.6250]]) tensor([[-8283.2578]])\n",
      "Loss / MAE on 1 val samples: 8283.882664548068\n",
      "Loss / MAE on 1 train samples: 27965.939104560748\n",
      "tensor([[0.6250]]) tensor([[-7658.3999]])\n",
      "Loss / MAE on 1 val samples: 7659.025003223322\n",
      "Loss / MAE on 1 train samples: 54692.809742232115\n",
      "tensor([[0.6250]]) tensor([[-6927.5488]])\n",
      "Loss / MAE on 1 val samples: 6928.1737853492095\n",
      "Loss / MAE on 1 train samples: 14704.315507062884\n",
      "tensor([[0.6250]]) tensor([[-6244.6064]])\n",
      "Loss / MAE on 1 val samples: 6245.231460882775\n",
      "Loss / MAE on 1 train samples: 17550.900306323318\n",
      "tensor([[0.6250]]) tensor([[-5739.6646]])\n",
      "Loss / MAE on 1 val samples: 5740.289539735779\n",
      "Loss / MAE on 1 train samples: 10686.135695084075\n",
      "tensor([[0.6250]]) tensor([[-4494.4136]])\n",
      "Loss / MAE on 1 val samples: 4495.038598276994\n",
      "Loss / MAE on 1 train samples: 6926.3741021637725\n",
      "tensor([[0.6250]]) tensor([[-2844.8010]])\n",
      "Loss / MAE on 1 val samples: 2845.4260665144684\n",
      "Loss / MAE on 1 train samples: 21506.337183781434\n",
      "tensor([[0.6250]]) tensor([[-3788.0554]])\n",
      "Loss / MAE on 1 val samples: 3788.680377123412\n",
      "Loss / MAE on 1 train samples: 2435.3414671301102\n",
      "tensor([[0.6250]]) tensor([[-4037.0969]])\n",
      "Loss / MAE on 1 val samples: 4037.721882448072\n",
      "Loss / MAE on 1 train samples: 14817.313959309562\n",
      "tensor([[0.6250]]) tensor([[-4178.0381]])\n",
      "Loss / MAE on 1 val samples: 4178.663183363789\n",
      "Loss / MAE on 1 train samples: 5405.080030556323\n",
      "tensor([[0.6250]]) tensor([[-5566.4219]])\n",
      "Loss / MAE on 1 val samples: 5567.046793408513\n",
      "Loss / MAE on 1 train samples: 10541.099344500315\n",
      "tensor([[0.6250]]) tensor([[-6493.3354]])\n",
      "Loss / MAE on 1 val samples: 6493.960578876346\n",
      "Loss / MAE on 1 train samples: 29962.015014848384\n",
      "tensor([[0.6250]]) tensor([[-11509.8867]])\n",
      "Loss / MAE on 1 val samples: 11510.511717556261\n",
      "Loss / MAE on 1 train samples: 18588.768240190097\n",
      "tensor([[0.6250]]) tensor([[-11127.1719]])\n",
      "Loss / MAE on 1 val samples: 11127.796906845488\n",
      "Loss / MAE on 1 train samples: 41338.161470365376\n",
      "tensor([[0.6250]]) tensor([[-10284.2441]])\n",
      "Loss / MAE on 1 val samples: 10284.869274813365\n",
      "Loss / MAE on 1 train samples: 7424.503481925576\n",
      "tensor([[0.6250]]) tensor([[-9841.0430]])\n",
      "Loss / MAE on 1 val samples: 9841.66815128411\n",
      "Loss / MAE on 1 train samples: 2032.6461548004363\n",
      "tensor([[0.6250]]) tensor([[-8509.2510]])\n",
      "Loss / MAE on 1 val samples: 8509.87614480963\n",
      "Loss / MAE on 1 train samples: 38413.12845239242\n",
      "tensor([[0.6250]]) tensor([[6901.1338]])\n",
      "Loss / MAE on 1 val samples: 6900.508676902015\n",
      "Loss / MAE on 1 train samples: 8404.230414927206\n",
      "tensor([[0.6250]]) tensor([[30004.4707]])\n",
      "Loss / MAE on 1 val samples: 30003.846153451726\n",
      "Loss / MAE on 1 train samples: 11330.193345658317\n",
      "tensor([[0.6250]]) tensor([[50239.8984]])\n",
      "Loss / MAE on 1 val samples: 50239.27388010301\n",
      "Loss / MAE on 1 train samples: 10451.10079429435\n",
      "tensor([[0.6250]]) tensor([[76903.2500]])\n",
      "Loss / MAE on 1 val samples: 76902.62476664889\n",
      "Loss / MAE on 1 train samples: 12826.295164865416\n",
      "tensor([[0.6250]]) tensor([[106340.0547]])\n",
      "Loss / MAE on 1 val samples: 106339.42817224475\n",
      "Loss / MAE on 1 train samples: 11083.21594940972\n",
      "tensor([[0.6250]]) tensor([[132459.2500]])\n",
      "Loss / MAE on 1 val samples: 132458.62629515678\n",
      "Loss / MAE on 1 train samples: 19395.207204358503\n",
      "tensor([[0.6250]]) tensor([[117687.5000]])\n",
      "Loss / MAE on 1 val samples: 117686.87294681594\n",
      "Loss / MAE on 1 train samples: 21677.949085706885\n",
      "tensor([[0.6250]]) tensor([[117812.1172]])\n",
      "Loss / MAE on 1 val samples: 117811.49308959632\n",
      "Loss / MAE on 1 train samples: 21611.167902845973\n",
      "tensor([[0.6250]]) tensor([[74618.7266]])\n",
      "Loss / MAE on 1 val samples: 74618.10268292809\n",
      "Loss / MAE on 1 train samples: 6671.460759974159\n",
      "tensor([[0.6250]]) tensor([[39172.2305]])\n",
      "Loss / MAE on 1 val samples: 39171.606043153246\n",
      "Loss / MAE on 1 train samples: 18208.532726787736\n",
      "tensor([[0.6250]]) tensor([[21361.2031]])\n",
      "Loss / MAE on 1 val samples: 21360.57826932595\n",
      "Loss / MAE on 1 train samples: 670.1469875059361\n",
      "tensor([[0.6250]]) tensor([[6064.4648]])\n",
      "Loss / MAE on 1 val samples: 6063.839707643994\n",
      "Loss / MAE on 1 train samples: 7376.847954389971\n",
      "tensor([[0.6250]]) tensor([[-12407.5156]])\n",
      "Loss / MAE on 1 val samples: 12408.140553684907\n",
      "Loss / MAE on 1 train samples: 629.8050763562497\n",
      "tensor([[0.6250]]) tensor([[-27876.1270]])\n",
      "Loss / MAE on 1 val samples: 27876.751604159334\n",
      "Loss / MAE on 1 train samples: 6007.515028396205\n",
      "tensor([[0.6250]]) tensor([[-44453.8789]])\n",
      "Loss / MAE on 1 val samples: 44454.503483899134\n",
      "Loss / MAE on 1 train samples: 42385.317696697755\n",
      "tensor([[0.6250]]) tensor([[-19559.2539]])\n",
      "Loss / MAE on 1 val samples: 19559.878936230663\n",
      "Loss / MAE on 1 train samples: 3223.4210266787018\n",
      "tensor([[0.6250]]) tensor([[-2933.7371]])\n",
      "Loss / MAE on 1 val samples: 2934.362111260299\n"
     ]
    }
   ],
   "source": [
    "# REGRESSION \n",
    "\n",
    "# Define callbacks\n",
    "# early_stop = EarlyStop(patience=args.patience, max_delta=args.max_delta)\n",
    "\n",
    "print(f\"Start training for {args.epochs} epochs\")\n",
    "\n",
    "min_val_metric = np.inf\n",
    "counter = 0 \n",
    "\n",
    "for epoch in range(args.epochs): \n",
    "    \n",
    "    mean_loss_epoch_train_mae = train_one_epoch(model, data_loader_train, optimizer, criterion, device, epoch, scaled=True, args=args) #loss_scaler, criterion\n",
    "    print(f\"Loss / MAE on {len(dataset_train)} train samples: {mean_loss_epoch_train_mae}\")\n",
    "\n",
    "    # mean_loss_epoch_val_bce, mean_loss_epoch_val_L1 = evaluate(model, data_loader_val, criterion, device, epoch, args=args) \n",
    "    target, output, mean_loss_epoch_val_mae = evaluate(model, data_loader_val, criterion, device, epoch, scaled=True, args=args) \n",
    "    print(target, output) \n",
    "    print(f\"Loss / MAE on {len(dataset_val)} val samples: {mean_loss_epoch_val_mae}\")\n",
    "    wandb.log({\"mean train MAE loss\": mean_loss_epoch_train_mae,\n",
    "               \"mean val MAE loss\": mean_loss_epoch_val_mae, \n",
    "               \"true age\": target, \n",
    "               \"guessed age\": output, \n",
    "               \"epoch\": epoch})\n",
    "    \n",
    "    # Early Stopping\n",
    "    # print(f\"Sufficient accuracy: {args.sufficient_accuracy}.\")\n",
    "    # print(f\"patience: {args.patience > -1}.\")\n",
    "    if args.patience > -1: \n",
    "        if mean_loss_epoch_val_mae < 0.1: #args.sufficient_accuracy: \n",
    "            break\n",
    "        # elif mean_loss_epoch_val_mae < min_val_metric: \n",
    "        #     min_val_metric = mean_loss_epoch_val_mae\n",
    "        #     counter == 0\n",
    "        # elif mean_loss_epoch_val_mae > min_val_metric: \n",
    "        #     counter += 1\n",
    "        #     if counter > args.patience:\n",
    "        #         print(f\"stopped early at epoch {epoch}.\")\n",
    "        #         break \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "- SDG works increadibly bad\n",
    "- first_shallow_conv_net and deep_conv_net have currently hardcoded eeg channels (n_channels = 61) and input time lengths (input_time_length == 100)\n",
    "- Created ShallowConvNet_Regression for regression, that does not have a final activation function (the one in the paper has a softmax) \n",
    "- age is now scaled to be between 0 and approx. 1 (linearly, by dividing by 100 >> scale instead to fit normal-ish distributon?. Done with \"self.y = self.y/100\")  \n",
    "- Final layer of DeepConvNet is currently hardcoded (self.linear_classification = nn.Linear(33200, 1)). Change that. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mae2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
