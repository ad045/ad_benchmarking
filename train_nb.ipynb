{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madrian-dendorfer\u001b[0m (\u001b[33madrian_s_playground\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import argparse \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import json as json\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "import models as models\n",
    "\n",
    "import wandb\n",
    "# from os import Path\n",
    "\n",
    "import models \n",
    "import datasets\n",
    "import dataset\n",
    "\n",
    "import numpy as np\n",
    "import time as time \n",
    "import util.misc as misc\n",
    "# from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "from util.callbacks import EarlyStop\n",
    "\n",
    "from util.engine_train import train_one_epoch, evaluate # evaluate_online\n",
    "\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser(\"NN training\")\n",
    "\n",
    "    parser.add_argument('--batch_size', default=64, type=int)\n",
    "    parser.add_argument('--epochs', default=400, type=int)\n",
    "    parser.add_argument('--acum_iter', default=1, type=int) \n",
    "\n",
    "    parser.add_argument('--model', default='shallow_conv_net', type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "    \n",
    "    # Model parameters\n",
    "    parser.add_argument('--input_channels', type=int, default=1, metavar='N',\n",
    "                        help='input channels')\n",
    "    parser.add_argument('--input_electrodes', type=int, default=61, metavar='N',\n",
    "                        help='input electrodes')\n",
    "    parser.add_argument('--time_steps', type=int, default=100, metavar='N',\n",
    "                        help='input length')\n",
    "    # parser.add_argument('--length_samples', default=200, \n",
    "    #                     help='length of samples') \n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument('--optimizer', type=str, default=\"adam_w\", \n",
    "                        help='optimizer type') \n",
    "    parser.add_argument('--lr', type=float, default=1e-3, metavar='LR',\n",
    "                        help='learning rate') \n",
    "\n",
    "    # Callback parameters\n",
    "    parser.add_argument('--patience', default=-1, type=float,\n",
    "                        help='Early stopping whether val is worse than train for specified nb of epochs (default: -1, i.e. no early stopping)')\n",
    "    parser.add_argument('--max_delta', default=0, type=float,\n",
    "                        help='Early stopping threshold (val has to be worse than (train+delta)) (default: 0)')\n",
    "\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument('--data_path', \n",
    "                        # default='_.pt',\n",
    "                        default=\"/vol/aimspace/users/dena/Documents/mae/data/lemon/data_raw_train.pt\",\n",
    "                        type=str,\n",
    "                        help='train dataset path')\n",
    "\n",
    "    parser.add_argument('--labels_path', \n",
    "                        # default='_.pt', \n",
    "                        default=\"/vol/aimspace/users/dena/Documents/ad_benchmarking/ad_benchmarking/data/labels_bin_train.pt\", #labels_raw_train.pt\",\n",
    "                        type=str,\n",
    "                        help='train labels path')\n",
    "    parser.add_argument('--val_data_path', \n",
    "                        # default='', \n",
    "                        default=\"/vol/aimspace/users/dena/Documents/mae/data/lemon/data_raw_val.pt\",\n",
    "                        type=str,\n",
    "                        help='validation dataset path')\n",
    "    parser.add_argument('--val_labels_path', \n",
    "                        # default='_.pt', \n",
    "                        default=\"/vol/aimspace/users/dena/Documents/ad_benchmarking/ad_benchmarking/data/labels_bin_val.pt\", # \"labels_raw_val.pt\"\n",
    "                        type=str,\n",
    "                        help='validation labels path')\n",
    "    parser.add_argument('--number_samples', default=1, type=int, # | str, \n",
    "                        help='number of samples on which network should train on. \"None\" means all samples.')\n",
    "    \n",
    "    \n",
    "    # Wandb parameters\n",
    "    parser.add_argument('--wandb', action='store_true', default=False)\n",
    "    parser.add_argument('--wandb_project', default='',\n",
    "                        help='project where to wandb log')\n",
    "    parser.add_argument('--wandb_id', default='', type=str,\n",
    "                        help='id of the current run')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "\n",
    "    # Saving Parameters\n",
    "    parser.add_argument('--output_dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    \n",
    "    # parser.add_argument('--mode', type=str, default=\"train\")\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(batch_size=64,\n",
    "    epochs=50,\n",
    "    acum_iter=1,\n",
    "    model='shallow_conv_net',\n",
    "    input_channels=1,\n",
    "    input_electrodes=61,\n",
    "    time_steps=100,\n",
    "    optimizer='adam_w',\n",
    "    lr=0.001,\n",
    "    patience=-1,\n",
    "    max_delta=0,\n",
    "    data_path='/vol/aimspace/users/dena/Documents/mae/data/lemon/data_raw_train.pt',\n",
    "    labels_path='/vol/aimspace/users/dena/Documents/ad_benchmarking/ad_benchmarking/data/labels_bin_train.pt',\n",
    "    val_data_path='/vol/aimspace/users/dena/Documents/mae/data/lemon/data_raw_val.pt',\n",
    "    val_labels_path='/vol/aimspace/users/dena/Documents/ad_benchmarking/ad_benchmarking/data/labels_bin_val.pt',\n",
    "    number_samples=1,\n",
    "    num_workers=4,\n",
    "    wandb=False,\n",
    "    wandb_project='',\n",
    "    wandb_id='',\n",
    "    device='cuda',\n",
    "    seed=0,\n",
    "    output_dir='')\n",
    "\n",
    "\n",
    "# Training set size:  1\n",
    "# Validation set size:  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  1\n",
      "Validation set size:  1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/vol/aimspace/users/dena/Documents/ad_benchmarking/ad_benchmarking/wandb/run-20240314_153959-tbkihtj6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adrian_s_playground/ad_benchmarking/runs/tbkihtj6' target=\"_blank\">buttermilk-mousse-4</a></strong> to <a href='https://wandb.ai/adrian_s_playground/ad_benchmarking' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adrian_s_playground/ad_benchmarking' target=\"_blank\">https://wandb.ai/adrian_s_playground/ad_benchmarking</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adrian_s_playground/ad_benchmarking/runs/tbkihtj6' target=\"_blank\">https://wandb.ai/adrian_s_playground/ad_benchmarking/runs/tbkihtj6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(args.device)\n",
    "\n",
    "# Fix the seed for reproducibility\n",
    "seed = args.seed \n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "dataset_train = dataset.EEGDataset(data_path=args.data_path, labels_path=args.labels_path, \n",
    "                            train=True, number_samples=args.number_samples, length_samples=args.time_steps,\n",
    "                            args=args)\n",
    "dataset_val = dataset.EEGDataset(data_path=args.data_path, labels_path=args.labels_path, \n",
    "                            train=True, number_samples=args.number_samples, length_samples=args.time_steps,\n",
    "                            args=args)\n",
    "\n",
    "print(\"Training set size: \", len(dataset_train))\n",
    "print(\"Validation set size: \", len(dataset_val))\n",
    "\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "sampler_train = torch.utils.data.RandomSampler(dataset_train) \n",
    "\n",
    "wandb.init(project=args.wandb_project, config=vars(args))\n",
    "\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train, \n",
    "    sampler=sampler_train,\n",
    "    # shuffle=True,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, \n",
    "    sampler=sampler_val,\n",
    "    # shuffle=False,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "model = models.__dict__[args.model](\n",
    "    n_channels=args.input_electrodes, \n",
    "    input_time_length=args.time_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for 400 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/home/dena/.conda/envs/mae2/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /opt/conda/conda-bld/pytorch_1695392026823/work/aten/src/ATen/native/Convolution.cpp:1008.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss / BCE on 1 train samples: 0.42121419310569763\n",
      "Loss / BCE on 1 val samples: 0.11425530910491943\n",
      "Loss / BCE on 1 train samples: 0.11415863037109375\n",
      "Loss / BCE on 1 val samples: 0.1018444299697876\n",
      "Loss / BCE on 1 train samples: 0.1018318235874176\n",
      "Loss / BCE on 1 val samples: 0.07844498753547668\n",
      "Loss / BCE on 1 train samples: 0.0784749686717987\n",
      "Loss / BCE on 1 val samples: 0.05960647389292717\n",
      "Loss / BCE on 1 train samples: 0.059766169637441635\n",
      "Loss / BCE on 1 val samples: 0.045776426792144775\n",
      "Loss / BCE on 1 train samples: 0.04564346745610237\n",
      "Loss / BCE on 1 val samples: 0.03543790802359581\n",
      "Loss / BCE on 1 train samples: 0.035524800419807434\n",
      "Loss / BCE on 1 val samples: 0.027769284322857857\n",
      "Loss / BCE on 1 train samples: 0.027635205537080765\n",
      "Loss / BCE on 1 val samples: 0.02194667048752308\n",
      "Loss / BCE on 1 train samples: 0.021810811012983322\n",
      "Loss / BCE on 1 val samples: 0.017477545887231827\n",
      "Loss / BCE on 1 train samples: 0.01750205084681511\n",
      "Loss / BCE on 1 val samples: 0.013972225598990917\n",
      "Loss / BCE on 1 train samples: 0.014149460941553116\n",
      "Loss / BCE on 1 val samples: 0.01121552288532257\n",
      "Loss / BCE on 1 train samples: 0.01129020843654871\n",
      "Loss / BCE on 1 val samples: 0.009087963961064816\n",
      "Loss / BCE on 1 train samples: 0.009148595854640007\n",
      "Loss / BCE on 1 val samples: 0.00739104300737381\n",
      "Loss / BCE on 1 train samples: 0.007499252911657095\n",
      "Loss / BCE on 1 val samples: 0.006220635026693344\n",
      "Loss / BCE on 1 train samples: 0.006234729662537575\n",
      "Loss / BCE on 1 val samples: 0.005112222395837307\n",
      "Loss / BCE on 1 train samples: 0.005136246792972088\n",
      "Loss / BCE on 1 val samples: 0.0040908572264015675\n",
      "Loss / BCE on 1 train samples: 0.004211819265037775\n",
      "Loss / BCE on 1 val samples: 0.0033708936534821987\n",
      "Loss / BCE on 1 train samples: 0.003516232594847679\n",
      "Loss / BCE on 1 val samples: 0.0029124668799340725\n",
      "Loss / BCE on 1 train samples: 0.002805349649861455\n",
      "Loss / BCE on 1 val samples: 0.002368988934904337\n",
      "Loss / BCE on 1 train samples: 0.0023009406868368387\n",
      "Loss / BCE on 1 val samples: 0.0020182218868285418\n",
      "Loss / BCE on 1 train samples: 0.0019584987312555313\n",
      "Loss / BCE on 1 val samples: 0.0016879969043657184\n",
      "Loss / BCE on 1 train samples: 0.0016105023678392172\n",
      "Loss / BCE on 1 val samples: 0.0014175683027133346\n",
      "Loss / BCE on 1 train samples: 0.0013189068995416164\n",
      "Loss / BCE on 1 val samples: 0.0011522253043949604\n",
      "Loss / BCE on 1 train samples: 0.001189820235595107\n",
      "Loss / BCE on 1 val samples: 0.0009588426328264177\n",
      "Loss / BCE on 1 train samples: 0.0009509673109278083\n",
      "Loss / BCE on 1 val samples: 0.0008050462929531932\n",
      "Loss / BCE on 1 train samples: 0.0008132783696055412\n",
      "Loss / BCE on 1 val samples: 0.0006337385275401175\n",
      "Loss / BCE on 1 train samples: 0.000687716412357986\n",
      "Loss / BCE on 1 val samples: 0.00055250886362046\n",
      "Loss / BCE on 1 train samples: 0.0005390308215282857\n",
      "Loss / BCE on 1 val samples: 0.00047993252519518137\n",
      "Loss / BCE on 1 train samples: 0.00044111203169450164\n",
      "Loss / BCE on 1 val samples: 0.00042787406709976494\n",
      "Loss / BCE on 1 train samples: 0.0003959128225687891\n",
      "Loss / BCE on 1 val samples: 0.00034672071342356503\n",
      "Loss / BCE on 1 train samples: 0.0003072495455853641\n",
      "Loss / BCE on 1 val samples: 0.0002568693889770657\n",
      "Loss / BCE on 1 train samples: 0.0002998563286382705\n",
      "Loss / BCE on 1 val samples: 0.0002488803584128618\n",
      "Loss / BCE on 1 train samples: 0.00022717910178471357\n",
      "Loss / BCE on 1 val samples: 0.0002185941266361624\n",
      "Loss / BCE on 1 train samples: 0.00021513630053959787\n",
      "Loss / BCE on 1 val samples: 0.00015236108447425067\n",
      "Loss / BCE on 1 train samples: 0.00017507416487205774\n",
      "Loss / BCE on 1 val samples: 0.0001305426994804293\n",
      "Loss / BCE on 1 train samples: 0.00015820324188098311\n",
      "Loss / BCE on 1 val samples: 0.00011563969746930525\n",
      "Loss / BCE on 1 train samples: 0.00012577371671795845\n",
      "Loss / BCE on 1 val samples: 8.523827273165807e-05\n",
      "Loss / BCE on 1 train samples: 8.559592970414087e-05\n",
      "Loss / BCE on 1 val samples: 6.84284750605002e-05\n",
      "Loss / BCE on 1 train samples: 6.902455788804218e-05\n",
      "Loss / BCE on 1 val samples: 5.996406980557367e-05\n",
      "Loss / BCE on 1 train samples: 7.224344153655693e-05\n",
      "Loss / BCE on 1 val samples: 4.971151065547019e-05\n",
      "Loss / BCE on 1 train samples: 5.3049541747896e-05\n",
      "Loss / BCE on 1 val samples: 5.4241696489043534e-05\n",
      "Loss / BCE on 1 train samples: 3.95782662963029e-05\n",
      "Loss / BCE on 1 val samples: 3.933984044124372e-05\n",
      "Loss / BCE on 1 train samples: 4.386998261907138e-05\n",
      "Loss / BCE on 1 val samples: 3.60018530045636e-05\n",
      "Loss / BCE on 1 train samples: 2.9802766221109778e-05\n",
      "Loss / BCE on 1 val samples: 3.0041192076168954e-05\n",
      "Loss / BCE on 1 train samples: 3.230623769923113e-05\n",
      "Loss / BCE on 1 val samples: 3.0756469641346484e-05\n",
      "Loss / BCE on 1 train samples: 3.182938598911278e-05\n",
      "Loss / BCE on 1 val samples: 3.0637256713816896e-05\n",
      "Loss / BCE on 1 train samples: 2.7776150091085583e-05\n",
      "Loss / BCE on 1 val samples: 2.837221290974412e-05\n",
      "Loss / BCE on 1 train samples: 2.694166323635727e-05\n",
      "Loss / BCE on 1 val samples: 2.0384995877975598e-05\n",
      "Loss / BCE on 1 train samples: 2.0504208805505186e-05\n",
      "Loss / BCE on 1 val samples: 1.7285496141994372e-05\n",
      "Loss / BCE on 1 train samples: 2.062341991404537e-05\n",
      "Loss / BCE on 1 val samples: 1.978893851628527e-05\n",
      "Loss / BCE on 1 train samples: 2.30076584557537e-05\n",
      "Loss / BCE on 1 val samples: 1.8596821973915212e-05\n",
      "Loss / BCE on 1 train samples: 1.5378116586362012e-05\n",
      "Loss / BCE on 1 val samples: 1.9431303371675313e-05\n",
      "Loss / BCE on 1 train samples: 1.78815535036847e-05\n",
      "Loss / BCE on 1 val samples: 9.89442014542874e-06\n",
      "Loss / BCE on 1 train samples: 1.0848104466276709e-05\n",
      "Loss / BCE on 1 val samples: 8.583106136939023e-06\n",
      "Loss / BCE on 1 train samples: 1.430521751899505e-05\n",
      "Loss / BCE on 1 val samples: 1.1563368389033712e-05\n",
      "Loss / BCE on 1 train samples: 1.5258905477821827e-05\n",
      "Loss / BCE on 1 val samples: 1.3947584193374496e-05\n",
      "Loss / BCE on 1 train samples: 8.106264431262389e-06\n",
      "Loss / BCE on 1 val samples: 6.6757424974639434e-06\n",
      "Loss / BCE on 1 train samples: 1.0132840543519706e-05\n",
      "Loss / BCE on 1 val samples: 8.702316335984506e-06\n",
      "Loss / BCE on 1 train samples: 1.1444157280493528e-05\n",
      "Loss / BCE on 1 val samples: 7.271793037944008e-06\n",
      "Loss / BCE on 1 train samples: 5.841272468387615e-06\n",
      "Loss / BCE on 1 val samples: 9.179157132166438e-06\n",
      "Loss / BCE on 1 train samples: 6.7949526965094265e-06\n",
      "Loss / BCE on 1 val samples: 7.74863383412594e-06\n",
      "Loss / BCE on 1 train samples: 3.57628505298635e-06\n",
      "Loss / BCE on 1 val samples: 3.933914285880746e-06\n",
      "Loss / BCE on 1 train samples: 3.57628505298635e-06\n",
      "Loss / BCE on 1 val samples: 5.841272468387615e-06\n",
      "Loss / BCE on 1 train samples: 5.483642325998517e-06\n",
      "Loss / BCE on 1 val samples: 3.814704541582614e-06\n",
      "Loss / BCE on 1 train samples: 5.364432581700385e-06\n",
      "Loss / BCE on 1 val samples: 6.198902156029362e-06\n",
      "Loss / BCE on 1 train samples: 6.198902156029362e-06\n",
      "Loss / BCE on 1 val samples: 5.722062269342132e-06\n",
      "Loss / BCE on 1 train samples: 4.172333774477011e-06\n",
      "Loss / BCE on 1 val samples: 1.9073504518019035e-06\n",
      "Loss / BCE on 1 train samples: 5.364432581700385e-06\n",
      "Loss / BCE on 1 val samples: 3.695494797284482e-06\n",
      "Loss / BCE on 1 train samples: 2.98023678624304e-06\n",
      "Loss / BCE on 1 val samples: 1.4305124977909145e-06\n",
      "Loss / BCE on 1 train samples: 1.3113030945532955e-06\n",
      "Loss / BCE on 1 val samples: 3.814704541582614e-06\n",
      "Loss / BCE on 1 train samples: 1.0728841743912199e-06\n",
      "Loss / BCE on 1 val samples: 3.695494797284482e-06\n",
      "Loss / BCE on 1 train samples: 1.0728841743912199e-06\n",
      "Loss / BCE on 1 val samples: 1.0728841743912199e-06\n",
      "Loss / BCE on 1 train samples: 2.2649790025752736e-06\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 1.4305124977909145e-06\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 1.5497220147153712e-06\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 2.5033982637978625e-06\n",
      "Loss / BCE on 1 val samples: 2.145769485650817e-06\n",
      "Loss / BCE on 1 train samples: 1.7881409348774469e-06\n",
      "Loss / BCE on 1 val samples: 1.5497220147153712e-06\n",
      "Loss / BCE on 1 train samples: 1.1920935776288388e-06\n",
      "Loss / BCE on 1 val samples: 1.0728841743912199e-06\n",
      "Loss / BCE on 1 train samples: 1.1920935776288388e-06\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 1.0728841743912199e-06\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 1.0728841743912199e-06\n",
      "Loss / BCE on 1 val samples: 1.4305124977909145e-06\n",
      "Loss / BCE on 1 train samples: 1.1920935776288388e-06\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 val samples: 1.0728841743912199e-06\n",
      "Loss / BCE on 1 train samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 val samples: 1.0728841743912199e-06\n",
      "Loss / BCE on 1 train samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 val samples: 1.0728841743912199e-06\n",
      "Loss / BCE on 1 train samples: 1.0728841743912199e-06\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 1.0728841743912199e-06\n",
      "Loss / BCE on 1 val samples: 1.0728841743912199e-06\n",
      "Loss / BCE on 1 train samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 1.0728841743912199e-06\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 1.0728841743912199e-06\n",
      "Loss / BCE on 1 val samples: 1.0728841743912199e-06\n",
      "Loss / BCE on 1 train samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 1.0728841743912199e-06\n",
      "Loss / BCE on 1 val samples: 1.0728841743912199e-06\n",
      "Loss / BCE on 1 train samples: 1.0728841743912199e-06\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 val samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 train samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 val samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 train samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 val samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 train samples: 1.1920935776288388e-06\n",
      "Loss / BCE on 1 val samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 train samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 val samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 train samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 val samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 train samples: 1.1920935776288388e-06\n",
      "Loss / BCE on 1 val samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 train samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 1.0728841743912199e-06\n",
      "Loss / BCE on 1 val samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 train samples: 1.1920935776288388e-06\n",
      "Loss / BCE on 1 val samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 train samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 val samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 train samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 val samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 train samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 val samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 train samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 val samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 train samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 val samples: 1.0728841743912199e-06\n",
      "Loss / BCE on 1 train samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 val samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 train samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 val samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 train samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 val samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 train samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 val samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 train samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 val samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 train samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 val samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 train samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 val samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 train samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 val samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 train samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 val samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 train samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 val samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 train samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 val samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 train samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 val samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 train samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 val samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 train samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 val samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 train samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 val samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 train samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 val samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 train samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 val samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 train samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 val samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 train samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 val samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 train samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 val samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 train samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 val samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 train samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 val samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 train samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 val samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 train samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 val samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 train samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 val samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 train samples: 1.0728841743912199e-06\n",
      "Loss / BCE on 1 val samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 train samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 val samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 train samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 val samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 train samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 val samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 train samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 val samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 train samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 val samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 train samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 val samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 train samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 val samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 train samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 val samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 train samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 val samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 train samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 val samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 train samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 val samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 train samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 val samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 train samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 val samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 train samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 val samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 train samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 val samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 train samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 val samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 train samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 val samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 train samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 val samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 train samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 val samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 train samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 val samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 train samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 val samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 train samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 val samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 train samples: 9.536747711536009e-07\n",
      "Loss / BCE on 1 val samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 train samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 val samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 train samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 val samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 train samples: 8.344653679159819e-07\n",
      "Loss / BCE on 1 val samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 train samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 val samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 train samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 val samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 train samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 val samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 train samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 val samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 train samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 val samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 train samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 val samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 train samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 val samples: 7.152560215217818e-07\n",
      "Loss / BCE on 1 train samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 val samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 train samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 val samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 train samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 val samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 train samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 val samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 train samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 val samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 train samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 val samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 train samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 val samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 train samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 val samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 train samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 val samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 train samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 val samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 train samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 val samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 train samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 val samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 train samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 val samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 train samples: 5.960466182841628e-07\n",
      "Loss / BCE on 1 val samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 train samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 val samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 train samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 val samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 train samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 val samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 train samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 val samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 train samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 val samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 train samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 val samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 train samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 val samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 train samples: 3.576279254957626e-07\n",
      "Loss / BCE on 1 val samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 train samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 val samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 4.768372718899627e-07\n",
      "Loss / BCE on 1 train samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 val samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 2.3841860752327193e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 train samples: 1.1920930376163597e-07\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n",
      "Loss / BCE on 1 val samples: 0.0\n",
      "Loss / BCE on 1 train samples: 0.0\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "# eval_criterion = \"bce\"\n",
    "# criterion = nn.BCELoss()  # !!!! \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.95))\n",
    "\n",
    "# Define callbacks\n",
    "early_stop = EarlyStop(patience=args.patience, max_delta=args.max_delta)\n",
    "\n",
    "print(f\"Start training for {args.epochs} epochs\")\n",
    "\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    mean_loss_epoch_train = train_one_epoch(model, data_loader_train, optimizer, device, epoch, args=args) #loss_scaler, criterion\n",
    "    print(f\"Loss / BCE on {len(dataset_train)} train samples: {mean_loss_epoch_train}\")\n",
    "\n",
    "    mean_loss_epoch_val = evaluate(model, data_loader_val, device, epoch, args=args) \n",
    "    print(f\"Loss / BCE on {len(dataset_val)} val samples: {mean_loss_epoch_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mae2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
