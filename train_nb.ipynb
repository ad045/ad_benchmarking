{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madrian-dendorfer\u001b[0m (\u001b[33madrian_s_playground\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import argparse \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import json as json\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "import models as models\n",
    "\n",
    "import wandb\n",
    "# from os import Path\n",
    "\n",
    "import models \n",
    "import datasets\n",
    "import dataset\n",
    "\n",
    "import numpy as np\n",
    "import time as time \n",
    "import util.misc as misc\n",
    "# from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "from util.callbacks import EarlyStop\n",
    "\n",
    "from util.engine_train import train_one_epoch, evaluate # evaluate_online\n",
    "\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser(\"NN training\")\n",
    "\n",
    "    parser.add_argument('--batch_size', default=64, type=int)\n",
    "    parser.add_argument('--epochs', default=400, type=int)\n",
    "    parser.add_argument('--acum_iter', default=1, type=int) \n",
    "\n",
    "    parser.add_argument('--model', default='shallow_conv_net', type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "    \n",
    "    # Model parameters\n",
    "    parser.add_argument('--input_channels', type=int, default=1, metavar='N',\n",
    "                        help='input channels')\n",
    "    parser.add_argument('--input_electrodes', type=int, default=61, metavar='N',\n",
    "                        help='input electrodes')\n",
    "    parser.add_argument('--time_steps', type=int, default=100, metavar='N',\n",
    "                        help='input length')\n",
    "    # parser.add_argument('--length_samples', default=200, \n",
    "    #                     help='length of samples') \n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument('--optimizer', type=str, default=\"adam_w\", \n",
    "                        help='optimizer type') \n",
    "    parser.add_argument('--lr', type=float, default=1e-3, metavar='LR',\n",
    "                        help='learning rate') \n",
    "\n",
    "    # Callback parameters\n",
    "    parser.add_argument('--patience', default=-1, type=float,\n",
    "                        help='Early stopping whether val is worse than train for specified nb of epochs (default: -1, i.e. no early stopping)')\n",
    "    parser.add_argument('--max_delta', default=0, type=float,\n",
    "                        help='Early stopping threshold (val has to be worse than (train+delta)) (default: 0)')\n",
    "\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument('--data_path', \n",
    "                        # default='_.pt',\n",
    "                        default=\"/vol/aimspace/users/dena/Documents/mae/data/lemon/data_raw_train.pt\",\n",
    "                        type=str,\n",
    "                        help='train dataset path')\n",
    "\n",
    "    parser.add_argument('--labels_path', \n",
    "                        # default='_.pt', \n",
    "                        default=\"/vol/aimspace/users/dena/Documents/ad_benchmarking/ad_benchmarking/data/labels_bin_train.pt\", #labels_raw_train.pt\",\n",
    "                        type=str,\n",
    "                        help='train labels path')\n",
    "    parser.add_argument('--val_data_path', \n",
    "                        # default='', \n",
    "                        default=\"/vol/aimspace/users/dena/Documents/mae/data/lemon/data_raw_val.pt\",\n",
    "                        type=str,\n",
    "                        help='validation dataset path')\n",
    "    parser.add_argument('--val_labels_path', \n",
    "                        # default='_.pt', \n",
    "                        default=\"/vol/aimspace/users/dena/Documents/ad_benchmarking/ad_benchmarking/data/labels_bin_val.pt\", # \"labels_raw_val.pt\"\n",
    "                        type=str,\n",
    "                        help='validation labels path')\n",
    "    parser.add_argument('--number_samples', default=1, type=int, # | str, \n",
    "                        help='number of samples on which network should train on. \"None\" means all samples.')\n",
    "    \n",
    "    \n",
    "    # Wandb parameters\n",
    "    parser.add_argument('--wandb', action='store_true', default=False)\n",
    "    parser.add_argument('--wandb_project', default='',\n",
    "                        help='project where to wandb log')\n",
    "    parser.add_argument('--wandb_id', default='', type=str,\n",
    "                        help='id of the current run')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "\n",
    "    # Saving Parameters\n",
    "    parser.add_argument('--output_dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    \n",
    "    # parser.add_argument('--mode', type=str, default=\"train\")\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(batch_size=64,\n",
    "    epochs=200,\n",
    "    acum_iter=1,\n",
    "    model='first_shallow_conv_net_regression', #shallow_conv_net',  # deep_conv_net, simple_classifier\n",
    "    input_channels=1,\n",
    "    input_electrodes=61,\n",
    "    time_steps=100,\n",
    "    optimizer='adamw', #'adam_w',\n",
    "    criterion='mse',   \n",
    "    lr=0.012,\n",
    "    patience=100,\n",
    "    sufficient_accuracy=0.001, #-np.inf, \n",
    "    max_delta=0,\n",
    "    data_path='/vol/aimspace/users/dena/Documents/mae/data/lemon/data_raw_train.pt',\n",
    "    # Classification\n",
    "    # labels_path='/vol/aimspace/users/dena/Documents/ad_benchmarking/ad_benchmarking/data/labels_bin_train.pt',\n",
    "    #Regression\n",
    "    labels_path='/u/home/dena/Documents/mae/data/lemon/labels_raw_train.pt',\n",
    "    val_data_path='/vol/aimspace/users/dena/Documents/mae/data/lemon/data_raw_val.pt',\n",
    "    val_labels_path='/vol/aimspace/users/dena/Documents/ad_benchmarking/ad_benchmarking/data/labels_bin_val.pt',\n",
    "    number_samples=1, #16, #64,\n",
    "    num_workers=4,\n",
    "    wandb=False,\n",
    "    wandb_project='',\n",
    "    wandb_id='',\n",
    "    device='cpu', #cuda',\n",
    "    seed=0,\n",
    "    output_dir='')\n",
    "\n",
    "\n",
    "# Training set size:  1\n",
    "# Validation set size:  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[62.5000],\n",
       "        [22.5000],\n",
       "        [22.5000],\n",
       "        [32.5000],\n",
       "        [27.5000],\n",
       "        [22.5000],\n",
       "        [62.5000],\n",
       "        [22.5000],\n",
       "        [27.5000],\n",
       "        [67.5000],\n",
       "        [22.5000],\n",
       "        [22.5000],\n",
       "        [27.5000],\n",
       "        [72.5000],\n",
       "        [22.5000],\n",
       "        [72.5000],\n",
       "        [67.5000],\n",
       "        [22.5000],\n",
       "        [27.5000],\n",
       "        [22.5000],\n",
       "        [67.5000],\n",
       "        [22.5000],\n",
       "        [27.5000],\n",
       "        [27.5000],\n",
       "        [22.5000],\n",
       "        [22.5000],\n",
       "        [27.5000],\n",
       "        [22.5000],\n",
       "        [27.5000],\n",
       "        [62.5000],\n",
       "        [22.5000],\n",
       "        [22.5000],\n",
       "        [72.5000],\n",
       "        [22.5000],\n",
       "        [32.5000],\n",
       "        [22.5000],\n",
       "        [62.5000],\n",
       "        [22.5000],\n",
       "        [22.5000],\n",
       "        [27.5000],\n",
       "        [22.5000],\n",
       "        [67.5000],\n",
       "        [27.5000],\n",
       "        [62.5000],\n",
       "        [22.5000],\n",
       "        [27.5000],\n",
       "        [22.5000],\n",
       "        [72.5000],\n",
       "        [27.5000],\n",
       "        [67.5000],\n",
       "        [77.5000],\n",
       "        [27.5000],\n",
       "        [22.5000],\n",
       "        [72.5000],\n",
       "        [22.5000],\n",
       "        [62.5000],\n",
       "        [27.5000],\n",
       "        [22.5000],\n",
       "        [27.5000],\n",
       "        [67.5000],\n",
       "        [77.5000],\n",
       "        [72.5000],\n",
       "        [22.5000],\n",
       "        [22.5000],\n",
       "        [32.5000],\n",
       "        [22.5000],\n",
       "        [27.5000],\n",
       "        [72.5000],\n",
       "        [27.5000],\n",
       "        [62.5000],\n",
       "        [27.5000],\n",
       "        [67.5000],\n",
       "        [37.5000],\n",
       "        [72.5000],\n",
       "        [27.5000],\n",
       "        [27.5000],\n",
       "        [27.5000],\n",
       "        [22.5000],\n",
       "        [27.5000],\n",
       "        [22.5000],\n",
       "        [67.5000],\n",
       "        [27.5000],\n",
       "        [32.5000],\n",
       "        [27.5000],\n",
       "        [67.5000],\n",
       "        [22.5000],\n",
       "        [22.5000],\n",
       "        [57.5000],\n",
       "        [27.5000],\n",
       "        [67.5000],\n",
       "        [22.5000],\n",
       "        [72.5000],\n",
       "        [27.5000],\n",
       "        [22.5000],\n",
       "        [62.5000],\n",
       "        [67.5000],\n",
       "        [22.5000],\n",
       "        [62.5000],\n",
       "        [67.5000],\n",
       "        [72.5000],\n",
       "        [72.5000],\n",
       "        [32.5000],\n",
       "        [27.5000],\n",
       "        [32.5000],\n",
       "        [22.5000],\n",
       "        [22.5000],\n",
       "        [27.5000],\n",
       "        [32.5000],\n",
       "        [77.5000],\n",
       "        [22.5000],\n",
       "        [22.5000],\n",
       "        [22.5000],\n",
       "        [22.5000],\n",
       "        [22.5000],\n",
       "        [22.5000],\n",
       "        [22.5000],\n",
       "        [22.5000],\n",
       "        [22.5000],\n",
       "        [27.5000],\n",
       "        [67.5000],\n",
       "        [27.5000],\n",
       "        [72.5000],\n",
       "        [22.5000],\n",
       "        [22.5000],\n",
       "        [27.5000],\n",
       "        [27.5000],\n",
       "        [57.5000],\n",
       "        [62.5000],\n",
       "        [27.5000],\n",
       "        [22.5000],\n",
       "        [67.5000],\n",
       "        [22.5000],\n",
       "        [27.5000],\n",
       "        [27.5000],\n",
       "        [22.5000],\n",
       "        [67.5000],\n",
       "        [27.5000],\n",
       "        [72.5000],\n",
       "        [22.5000],\n",
       "        [27.5000],\n",
       "        [72.5000],\n",
       "        [27.5000],\n",
       "        [22.5000],\n",
       "        [62.5000],\n",
       "        [22.5000],\n",
       "        [22.5000],\n",
       "        [27.5000],\n",
       "        [67.5000],\n",
       "        [67.5000],\n",
       "        [27.5000]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.load(args.labels_path, map_location=torch.device('cpu')) # load to ram\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  1\n",
      "Validation set size:  1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/vol/aimspace/users/dena/Documents/ad_benchmarking/ad_benchmarking/wandb/run-20240318_130505-tkuvnl2u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adrian_s_playground/ad_benchmarking/runs/tkuvnl2u' target=\"_blank\">scarlet-gorge-55</a></strong> to <a href='https://wandb.ai/adrian_s_playground/ad_benchmarking' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adrian_s_playground/ad_benchmarking' target=\"_blank\">https://wandb.ai/adrian_s_playground/ad_benchmarking</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adrian_s_playground/ad_benchmarking/runs/tkuvnl2u' target=\"_blank\">https://wandb.ai/adrian_s_playground/ad_benchmarking/runs/tkuvnl2u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print('job dir: {}'.format(os.path.dirname(os.path.realpath(__file__))))\n",
    "# print(\"{}\".format(args).replace(', ', ',\\n'))\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "# Fix the seed for reproducibility\n",
    "seed = args.seed \n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "dataset_train = dataset.EEGDataset(data_path=args.data_path, labels_path=args.labels_path, \n",
    "                            train=True, number_samples=args.number_samples, length_samples=args.time_steps,\n",
    "                            args=args)\n",
    "dataset_val = dataset.EEGDataset(data_path=args.data_path, labels_path=args.labels_path, \n",
    "                            train=True, number_samples=args.number_samples, length_samples=args.time_steps,\n",
    "                            args=args)\n",
    "\n",
    "print(\"Training set size: \", len(dataset_train))\n",
    "print(\"Validation set size: \", len(dataset_val))\n",
    "\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "sampler_train = torch.utils.data.RandomSampler(dataset_train) \n",
    "\n",
    "# # wandb logging\n",
    "# if args.wandb == True:\n",
    "#     config = vars(args)\n",
    "#     if args.wandb_id:\n",
    "#         wandb.init(project=args.wandb_project, id=args.wandb_id, config=config)\n",
    "#     else:\n",
    "#         wandb.init(project=args.wandb_project, config=config)\n",
    "wandb.init(project=args.wandb_project, config=vars(args))\n",
    "\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train, \n",
    "    sampler=sampler_train,\n",
    "    # shuffle=True,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    "    # pin_memory=args.pin_mem,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, \n",
    "    sampler=sampler_val,\n",
    "    # shuffle=False,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    "    # pin_memory=args.pin_mem,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "model = models.__dict__[args.model](\n",
    "    n_channels=args.input_electrodes, \n",
    "    input_time_length=args.time_steps, \n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# eval_criterion = \"bce\"\n",
    "if args.criterion == \"bce\": \n",
    "    criterion = torch.nn.BCELoss() # For classification\n",
    "\n",
    "elif args.criterion == \"mae\": \n",
    "    criterion = torch.nn.L1Loss() # For regression \n",
    "\n",
    "elif args.criterion == \"mse\": \n",
    "    criterion = torch.nn.MSELoss() # For regression \n",
    "\n",
    "\n",
    "\n",
    "if args.optimizer == \"sgd\":\n",
    "    optimizer = optim.SGD(model.parameters(),\n",
    "                            lr=args.lr, momentum=0.9)\n",
    "elif args.optimizer == \"adam\":\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                            lr=args.lr)\n",
    "elif args.optimizer == \"adamw\": \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.95))\n",
    "\n",
    "else: \n",
    "    print(\"Attention: No optimier chosen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in enumerate(data_loader_val): \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[62.5000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now: \n",
    "data_loader_train = data_loader_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFICATION\n",
    "\n",
    "# # Define callbacks\n",
    "# # early_stop = EarlyStop(patience=args.patience, max_delta=args.max_delta)\n",
    "\n",
    "# print(f\"Start training for {args.epochs} epochs\")\n",
    "\n",
    "# min_val_metric = np.inf\n",
    "# counter = 0 \n",
    "\n",
    "# for epoch in range(args.epochs): \n",
    "    \n",
    "#     mean_loss_epoch_train_bce, mean_loss_epoch_train_L1 = train_one_epoch(model, data_loader_train, optimizer, criterion, device, epoch, args=args) #loss_scaler, criterion\n",
    "#     print(f\"Loss / BCE on {len(dataset_train)} train samples: {mean_loss_epoch_train_bce}\")\n",
    "\n",
    "#     # mean_loss_epoch_val_bce, mean_loss_epoch_val_L1 = evaluate(model, data_loader_val, criterion, device, epoch, args=args) \n",
    "#     target, output, mean_loss_epoch_val_bce, mean_loss_epoch_val_L1 = evaluate(model, data_loader_val, criterion, device, epoch, args=args) \n",
    "#     print(target, output) \n",
    "#     print(f\"Loss / BCE on {len(dataset_val)} val samples BCE: {mean_loss_epoch_val_bce}, val samples MAE: {mean_loss_epoch_val_L1}\")\n",
    "#     wandb.log({\"mean train BCE loss\": mean_loss_epoch_train_bce,\n",
    "#                \"mean train MAE loss\": mean_loss_epoch_train_L1, \n",
    "#                \"mean val BCE loss\": mean_loss_epoch_val_bce, \n",
    "#                \"mean val MAE loss\": mean_loss_epoch_val_L1, \n",
    "#                \"epoch\": epoch})\n",
    "    \n",
    "#     # Early Stopping\n",
    "#     print(f\"Sufficient accuracy: {args.sufficient_accuracy}.\")\n",
    "#     print(f\"patience: {args.patience > -1}.\")\n",
    "#     print(f\"stuff: {mean_loss_epoch_train_L1 < args.sufficient_accuracy}.\")\n",
    "#     if args.patience > -1: \n",
    "#         if mean_loss_epoch_train_L1 < args.sufficient_accuracy: \n",
    "#             break\n",
    "#         elif mean_loss_epoch_train_L1 < min_val_metric: \n",
    "#             min_val_metric = mean_loss_epoch_train_L1\n",
    "#             counter == 0\n",
    "#         elif mean_loss_epoch_train_L1 > min_val_metric: \n",
    "#             counter += 1\n",
    "#             if counter > args.patience:\n",
    "#                 print(f\"stopped early at epoch {epoch}.\")\n",
    "#                 break \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for 200 epochs\n",
      "Loss / MAE on 1 train samples: 61.94758605441176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/home/dena/.conda/envs/mae2/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /opt/conda/conda-bld/pytorch_1695392026823/work/aten/src/ATen/native/Convolution.cpp:1008.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: tensor([[62.5000]]), output: tensor([[0.9971]])\n",
      "tensor([[62.5000]]) tensor([[0.9971]])\n",
      "Loss / MAE on 1 val samples: 61.50289190672765\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss / MAE on 1 train samples: 61.96462298725883\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9998]])\n",
      "tensor([[62.5000]]) tensor([[0.9998]])\n",
      "Loss / MAE on 1 val samples: 61.50017069971229\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.92935980186417\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500011909297626\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.88625805093466\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.5\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.816974844343925\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.5\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.85598666206449\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.5\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.77109133412955\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.5\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.67234410449914\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.5\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.65476707225788\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.5\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.600884290020545\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.5\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.54620263341801\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.5\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.551783648506074\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500003969766134\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.555942306191085\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500003969766134\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.54655369299223\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500007939532004\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.52510166400073\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500003969766134\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.51535512386826\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.5\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.511042898308105\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.5\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.5090523857312\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.5\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.51000299748001\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500003969766134\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50648427135041\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500003969766134\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50690105145215\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500003969766134\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50378108613018\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500011909297626\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.502727168953655\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500033743003954\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50177445991904\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9999]])\n",
      "tensor([[62.5000]]) tensor([[0.9999]])\n",
      "Loss / MAE on 1 val samples: 61.50007939527396\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50141123571373\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9996]])\n",
      "tensor([[62.5000]]) tensor([[0.9996]])\n",
      "Loss / MAE on 1 val samples: 61.500418808914425\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50114526690235\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9997]])\n",
      "tensor([[62.5000]]) tensor([[0.9997]])\n",
      "Loss / MAE on 1 val samples: 61.500301701495744\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.501000372961414\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9979]])\n",
      "tensor([[62.5000]]) tensor([[0.9979]])\n",
      "Loss / MAE on 1 val samples: 61.502097985683584\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50096464563585\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9988]])\n",
      "tensor([[62.5000]]) tensor([[0.9988]])\n",
      "Loss / MAE on 1 val samples: 61.501228630384894\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500881281795465\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9965]])\n",
      "tensor([[62.5000]]) tensor([[0.9965]])\n",
      "Loss / MAE on 1 val samples: 61.503529020953344\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50071653863575\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9981]])\n",
      "tensor([[62.5000]]) tensor([[0.9981]])\n",
      "Loss / MAE on 1 val samples: 61.5018915645334\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500637144184324\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9987]])\n",
      "tensor([[62.5000]]) tensor([[0.9987]])\n",
      "Loss / MAE on 1 val samples: 61.501343751198846\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50061134096555\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9972]])\n",
      "tensor([[62.5000]]) tensor([[0.9972]])\n",
      "Loss / MAE on 1 val samples: 61.50276091046137\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500525991780144\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9996]])\n",
      "tensor([[62.5000]]) tensor([[0.9996]])\n",
      "Loss / MAE on 1 val samples: 61.50041483917507\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50049224904627\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9984]])\n",
      "tensor([[62.5000]]) tensor([[0.9984]])\n",
      "Loss / MAE on 1 val samples: 61.50157597701257\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500412854305296\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9977]])\n",
      "tensor([[62.5000]]) tensor([[0.9977]])\n",
      "Loss / MAE on 1 val samples: 61.50228455805016\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50037315689637\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9914]])\n",
      "tensor([[62.5000]]) tensor([[0.9914]])\n",
      "Loss / MAE on 1 val samples: 61.50856417440481\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50034735356683\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9973]])\n",
      "tensor([[62.5000]]) tensor([[0.9973]])\n",
      "Loss / MAE on 1 val samples: 61.50267754905574\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.5003056712424\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9981]])\n",
      "tensor([[62.5000]]) tensor([[0.9981]])\n",
      "Loss / MAE on 1 val samples: 61.501941185065455\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50029773174883\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9983]])\n",
      "tensor([[62.5000]]) tensor([[0.9983]])\n",
      "Loss / MAE on 1 val samples: 61.50167124888762\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50026001914036\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9984]])\n",
      "tensor([[62.5000]]) tensor([[0.9984]])\n",
      "Loss / MAE on 1 val samples: 61.50159781016364\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500263988889714\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9995]])\n",
      "tensor([[62.5000]]) tensor([[0.9995]])\n",
      "Loss / MAE on 1 val samples: 61.50047637010627\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.5003056712424\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9967]])\n",
      "tensor([[62.5000]]) tensor([[0.9967]])\n",
      "Loss / MAE on 1 val samples: 61.503292832960376\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50024017038978\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9994]])\n",
      "tensor([[62.5000]]) tensor([[0.9994]])\n",
      "Loss / MAE on 1 val samples: 61.50064905335857\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50019451823913\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9925]])\n",
      "tensor([[62.5000]]) tensor([[0.9925]])\n",
      "Loss / MAE on 1 val samples: 61.50746866171518\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50021833675674\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9989]])\n",
      "tensor([[62.5000]]) tensor([[0.9989]])\n",
      "Loss / MAE on 1 val samples: 61.50105197906171\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50024414014041\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9989]])\n",
      "tensor([[62.5000]]) tensor([[0.9989]])\n",
      "Loss / MAE on 1 val samples: 61.50108373664037\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50014886605459\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9988]])\n",
      "tensor([[62.5000]]) tensor([[0.9988]])\n",
      "Loss / MAE on 1 val samples: 61.50121275163503\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50013695678348\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.50004168253161\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500168714834636\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9999]])\n",
      "tensor([[62.5000]]) tensor([[0.9999]])\n",
      "Loss / MAE on 1 val samples: 61.50013298702593\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50019451823913\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.50004565229505\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50014092654077\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9997]])\n",
      "tensor([[62.5000]]) tensor([[0.9997]])\n",
      "Loss / MAE on 1 val samples: 61.50028185275861\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500115123113794\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9999]])\n",
      "tensor([[62.5000]]) tensor([[0.9999]])\n",
      "Loss / MAE on 1 val samples: 61.500087334795715\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500103213836155\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9999]])\n",
      "tensor([[62.5000]]) tensor([[0.9999]])\n",
      "Loss / MAE on 1 val samples: 61.500087334795715\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50016077532339\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9999]])\n",
      "tensor([[62.5000]]) tensor([[0.9999]])\n",
      "Loss / MAE on 1 val samples: 61.50011115335484\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50012901726813\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9998]])\n",
      "tensor([[62.5000]]) tensor([[0.9998]])\n",
      "Loss / MAE on 1 val samples: 61.50020642749909\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50009527431644\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9999]])\n",
      "tensor([[62.5000]]) tensor([[0.9999]])\n",
      "Loss / MAE on 1 val samples: 61.500061531346255\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500087334795715\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9998]])\n",
      "tensor([[62.5000]]) tensor([[0.9998]])\n",
      "Loss / MAE on 1 val samples: 61.500168714834636\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50007939527396\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9997]])\n",
      "tensor([[62.5000]]) tensor([[0.9997]])\n",
      "Loss / MAE on 1 val samples: 61.50026001914036\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500073440631965\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9999]])\n",
      "tensor([[62.5000]]) tensor([[0.9999]])\n",
      "Loss / MAE on 1 val samples: 61.500069470870315\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50007542551269\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9999]])\n",
      "tensor([[62.5000]]) tensor([[0.9999]])\n",
      "Loss / MAE on 1 val samples: 61.50009130455621\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50008336503496\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9999]])\n",
      "tensor([[62.5000]]) tensor([[0.9999]])\n",
      "Loss / MAE on 1 val samples: 61.50005756158384\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500069470870315\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9999]])\n",
      "tensor([[62.5000]]) tensor([[0.9999]])\n",
      "Loss / MAE on 1 val samples: 61.500049622058235\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50007939527396\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9999]])\n",
      "tensor([[62.5000]]) tensor([[0.9999]])\n",
      "Loss / MAE on 1 val samples: 61.500049622058235\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50006550110841\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9999]])\n",
      "tensor([[62.5000]]) tensor([[0.9999]])\n",
      "Loss / MAE on 1 val samples: 61.50009527431644\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50006550110841\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9999]])\n",
      "tensor([[62.5000]]) tensor([[0.9999]])\n",
      "Loss / MAE on 1 val samples: 61.50007542551269\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500069470870315\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.50004168253161\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500053591821164\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9998]])\n",
      "tensor([[62.5000]]) tensor([[0.9998]])\n",
      "Loss / MAE on 1 val samples: 61.500182608976864\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500053591821164\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.50004565229505\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500049622058235\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.50004168253161\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500053591821164\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.50004168253161\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500061531346255\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500049622058235\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500061531346255\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9999]])\n",
      "tensor([[62.5000]]) tensor([[0.9999]])\n",
      "Loss / MAE on 1 val samples: 61.50009527431644\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50004565229505\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500049622058235\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500049622058235\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9999]])\n",
      "tensor([[62.5000]]) tensor([[0.9999]])\n",
      "Loss / MAE on 1 val samples: 61.500061531346255\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50004168253161\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500033743003954\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50004168253161\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9999]])\n",
      "tensor([[62.5000]]) tensor([[0.9999]])\n",
      "Loss / MAE on 1 val samples: 61.500049622058235\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50004168253161\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9998]])\n",
      "tensor([[62.5000]]) tensor([[0.9998]])\n",
      "Loss / MAE on 1 val samples: 61.50016077532339\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500037712767906\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500037712767906\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500037712767906\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500037712767906\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500037712767906\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500029773239746\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500037712767906\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500049622058235\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500033743003954\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.50004565229505\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500033743003954\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500029773239746\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500033743003954\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9997]])\n",
      "tensor([[62.5000]]) tensor([[0.9997]])\n",
      "Loss / MAE on 1 val samples: 61.50026795863881\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500033743003954\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9998]])\n",
      "tensor([[62.5000]]) tensor([[0.9998]])\n",
      "Loss / MAE on 1 val samples: 61.500224291384704\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50004565229505\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500025803475275\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500033743003954\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500029773239746\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500029773239746\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500029773239746\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500033743003954\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500033743003954\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500025803475275\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500025803475275\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500029773239746\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500033743003954\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500025803475275\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500033743003954\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500025803475275\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.50004565229505\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500025803475275\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500019848828096\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500025803475275\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.50002381859295\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500025803475275\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500029773239746\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50002381859295\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.50002381859295\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50002381859295\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.50002381859295\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50002381859295\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.50002381859295\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50002381859295\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500019848828096\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50002381859295\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500019848828096\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50002381859295\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.50001587906299\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50002381859295\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500029773239746\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50002381859295\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9999]])\n",
      "tensor([[62.5000]]) tensor([[0.9999]])\n",
      "Loss / MAE on 1 val samples: 61.50009924407643\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500019848828096\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.50001587906299\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50002381859295\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500019848828096\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500019848828096\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500019848828096\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500025803475275\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.50002381859295\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500019848828096\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500029773239746\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.50002381859295\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.50002381859295\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500019848828096\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.500037712767906\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500019848828096\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.50002381859295\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500019848828096\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.50001587906299\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500019848828096\n",
      "Target: tensor([[62.5000]]), output: tensor([[1.0000]])\n",
      "tensor([[62.5000]]) tensor([[1.0000]])\n",
      "Loss / MAE on 1 val samples: 61.50001587906299\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "Loss / MAE on 1 train samples: 61.500025803475275\n",
      "Target: tensor([[62.5000]]), output: tensor([[0.9999]])\n",
      "tensor([[62.5000]]) tensor([[0.9999]])\n",
      "Loss / MAE on 1 val samples: 61.500107183595624\n",
      "Sufficient accuracy: 0.001.\n",
      "patience: True.\n",
      "stopped early at epoch 114.\n"
     ]
    }
   ],
   "source": [
    "# REGRESSION \n",
    "\n",
    "# Define callbacks\n",
    "# early_stop = EarlyStop(patience=args.patience, max_delta=args.max_delta)\n",
    "\n",
    "print(f\"Start training for {args.epochs} epochs\")\n",
    "\n",
    "min_val_metric = np.inf\n",
    "counter = 0 \n",
    "\n",
    "for epoch in range(args.epochs): \n",
    "    \n",
    "    mean_loss_epoch_train_mae = train_one_epoch(model, data_loader_train, optimizer, criterion, device, epoch, args=args) #loss_scaler, criterion\n",
    "    print(f\"Loss / MAE on {len(dataset_train)} train samples: {mean_loss_epoch_train_mae}\")\n",
    "\n",
    "    # mean_loss_epoch_val_bce, mean_loss_epoch_val_L1 = evaluate(model, data_loader_val, criterion, device, epoch, args=args) \n",
    "    target, output, mean_loss_epoch_val_mae = evaluate(model, data_loader_val, criterion, device, epoch, args=args) \n",
    "    print(target, output) \n",
    "    print(f\"Loss / MAE on {len(dataset_val)} val samples: {mean_loss_epoch_val_mae}\")\n",
    "    wandb.log({\"mean train BCE loss\": mean_loss_epoch_train_mae,\n",
    "               \"mean val BCE loss\": mean_loss_epoch_val_mae, \n",
    "               \"epoch\": epoch})\n",
    "    \n",
    "    # Early Stopping\n",
    "    print(f\"Sufficient accuracy: {args.sufficient_accuracy}.\")\n",
    "    print(f\"patience: {args.patience > -1}.\")\n",
    "    if args.patience > -1: \n",
    "        if mean_loss_epoch_val_mae < args.sufficient_accuracy: \n",
    "            break\n",
    "        elif mean_loss_epoch_val_mae < min_val_metric: \n",
    "            min_val_metric = mean_loss_epoch_val_mae\n",
    "            counter == 0\n",
    "        elif mean_loss_epoch_val_mae > min_val_metric: \n",
    "            counter += 1\n",
    "            if counter > args.patience:\n",
    "                print(f\"stopped early at epoch {epoch}.\")\n",
    "                break \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "- SDG works increadibly bad\n",
    "- first_shallow_conv_net and deep_conv_net have currently hardcoded eeg channels (n_channels = 61) and input time lengths (input_time_length == 100)\n",
    "- Created ShallowConvNet_Regression for regression, that does not have a final activation function (the one in the paper has a softmax) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block1_conv = 534-25+1 # same\n",
    "block1_conv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block1_conv = 510\n",
    "block1_max = block1_conv/3\n",
    "block1_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block1_max = 171\n",
    "block2_conv = (block1_max-10)+1 # valid\n",
    "print(block2_conv)\n",
    "block2_max = block2_conv/3 # valid\n",
    "block2_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.0\n",
      "15.0\n"
     ]
    }
   ],
   "source": [
    "block3_conv = block2_max-10+1\n",
    "print(block3_conv)\n",
    "block3_max = block3_conv/3\n",
    "print(block3_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mae2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
